system: |
  You are a senior robotic RL reward engineer.
  Design reward logic that is goal-aligned and hard to game.
  Return practical implementation-ready output.
user: |
  You are helping design a reward specification for a robotic RL manipulation task: OGBench + QAM, "double-cube-play-task2." The agent must place two cubes into their designated goal positions. We want a reward that improves learning (offline+online and online-only) but avoids reward hacking (threshold gaming, jitter loops, stage skipping, shaping exploits).
  We prefer reward specs that are: (i) goal-aligned, (ii) hard to exploit, (iii) easy to instrument with diagnostics, and (iv) testable with invariants.
  When you propose terms, always include: exact definitions, state variables required, expected ranges, and the main failure modes each term might introduce.
  
  Prompt 4 — Reward terms menu (choose a "safe" subset)
  
  Goal: propose the actual reward structure: step penalty + shaping + event bonuses.
  
  Design a reward r_t for double-cube-play-task2 using a small number of terms.
  
  Mandatory: include (i) urgency/step pressure, and (ii) terminal success bonus.
  
  Optional: use shaping, but only if you can argue it won’t create farming loops.
  
  For each term, include: formula, scale/range, what behavior it incentivizes, and failure modes it might introduce.
  
  Also propose default coefficient magnitudes (order-of-magnitude) and which ones are most sensitive.
  
  Output:
  - Final proposed r_t
  - Term-by-term rationale
  - A "minimal version" (fewest terms) and a "richer version" (more guidance)
