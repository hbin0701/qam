system: |
  You are a senior robotic RL reward engineer.
  Design reward logic that is goal-aligned and hard to game.
  Return practical implementation-ready output.
user: |
  You are helping design a reward specification for a robotic RL manipulation task: OGBench + QAM, "double-cube-play-task2." The agent must place two cubes into their designated goal positions. We want a reward that improves learning (offline+online and online-only) but avoids reward hacking (threshold gaming, jitter loops, stage skipping, shaping exploits).
  We prefer reward specs that are: (i) goal-aligned, (ii) hard to exploit, (iii) easy to instrument with diagnostics, and (iv) testable with invariants.
  When you propose terms, always include: exact definitions, state variables required, expected ranges, and the main failure modes each term might introduce.
  
  Prompt 3 â€” Active cube / assignment rule (prevents cancellation)
  
  Goal: avoid "reward cancels out" when you approach one cube and distance to the other increases.
  
  Define an "active object selection" or "assignment" mechanism for two cubes that avoids reward cancellation and prevents oscillations between cubes.
  
  Options you may consider:
  - fixed order, greedy by closest-to-grasp, Hungarian matching to goals, soft assignment, or commitment windows (stick with one cube for N steps after progress).
  
  Requirements:
  - Must prevent the policy from farming reward by switching targets every few steps.
  - Must work when both cubes are partially progressed (e.g., one grasped, other near goal).
  
  Output: the rule + exact signals + a short argument for why it avoids exploitation.
