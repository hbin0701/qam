system: |
  You are a senior robotic RL reward engineer.
  Design reward logic that is goal-aligned and hard to game.
  Return practical implementation-ready output.
user: |
  You are helping design a reward specification for a robotic RL manipulation task: OGBench + QAM, "double-cube-play-task2." The agent must place two cubes into their designated goal positions. We want a reward that improves learning (offline+online and online-only) but avoids reward hacking (threshold gaming, jitter loops, stage skipping, shaping exploits).
  We prefer reward specs that are: (i) goal-aligned, (ii) hard to exploit, (iii) easy to instrument with diagnostics, and (iv) testable with invariants.
  When you propose terms, always include: exact definitions, state variables required, expected ranges, and the main failure modes each term might introduce.
  
  Prompt 8 â€” Logging spec (what you must record to debug later)
  
  Goal: define what to log so Phase 2 diagnostics are possible.
  
  Define the minimal logging required to diagnose reward issues for double-cube-play-task2.
  
  Must include per-step: stage id, active cube id, per-term reward breakdown, cube-goal distances, grasp/contact flags, "in-hand" state, placement stability counters, and success predicate.
  
  Output: list of fields + brief reason each is necessary.
  
  After answering this prompt, produce implementation-ready Python reward code.
  
  Code requirements:
  - Target file style: qam/envs/rewards/*.py compatible.
  - Include a main callable similar to: compute_reward(obs, next_obs, info, state) -> (reward, state, diagnostics).
  - Include one-shot event latches in state.
  - Include active-cube commitment logic.
  - Include success and stability counters.
  - Include per-term diagnostic outputs.
  - Keep it deterministic and avoid placeholder pseudo-code.
  
  Return format:
  1) "DESIGN" section (concise but complete)
  2) "PYTHON_CODE" section with one fenced python block only.
