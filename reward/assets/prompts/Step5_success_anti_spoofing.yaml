system: |
  You are a senior robotic RL reward engineer.
  Design reward logic that is goal-aligned and hard to game.
  Return practical implementation-ready output.
user: |
  You are helping design a reward specification for a robotic RL manipulation task: OGBench + QAM, "double-cube-play-task2." The agent must place two cubes into their designated goal positions. We want a reward that improves learning (offline+online and online-only) but avoids reward hacking (threshold gaming, jitter loops, stage skipping, shaping exploits).
  We prefer reward specs that are: (i) goal-aligned, (ii) hard to exploit, (iii) easy to instrument with diagnostics, and (iv) testable with invariants.
  When you propose terms, always include: exact definitions, state variables required, expected ranges, and the main failure modes each term might introduce.
  
  Prompt 5 â€” Success definition & anti-spoofing
  
  Goal: success condition must match OGBench task success and be robust to transient contacts.
  
  Specify a robust success condition for placing two cubes in designated goals.
  
  Requirements:
  - Use "stable for K steps" and/or velocity thresholds to avoid momentary threshold hits.
  - Define how you detect cube-goal match (designated goal, not either goal).
  - Include any tolerance parameters and how to choose them.
  
  Output: success predicate + stability criteria + parameters.
