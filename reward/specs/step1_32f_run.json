{
  "system_prompt": "You are a senior robotic RL reward engineer.\nYou write robust, deterministic Python for manipulation tasks.\nReturn practical implementation-ready code only.",
  "user_prompt": "You are given:\n1) Environment implementation code (objects, state structure, signals).\n2) A short rollout video for behavior context.\n\nTask:\nGenerate Python code that defines subtask logic for this environment.\n\nYou must implement these functions:\n- propose_substages() -> list[str]\n- determine_subtask(state, memory) -> int\n- measure_subtask_progress(state, subtask_id, memory) -> float\n- determine_subtask_and_progress(state, memory=None) -> tuple[int, float]\n\nRequirements:\n- `determine_subtask_and_progress` returns:\n  - `subtask_id`: int index into propose_substages list\n  - `progress`: float in [0.0, 1.0] for current subtask completion\n- Must be deterministic and robust to noisy thresholds (use hysteresis/debounce where needed).\n- Must support multi-object tasks without progress cancellation.\n- No pseudocode; real Python code only.\n- Include helper functions if needed.\n- Keep dependencies minimal (`numpy` allowed).\n- You may call tools for geometry lookups:\n  - get_position(frame_num, object_name)\n  - get_dist_xy(x, y)\n  - get_dist_xyz(x, y)\n- For `get_position`, valid `object_name` values are provided at runtime from the pose JSON.\n- Use `frame_num` equal to the provided `source_frame_index` shown next to each sampled frame.\n\nOutput format (STRICT):\n- Return exactly one fenced python block and nothing else.\n\nEnvironment code:\n```python\n\"\"\"\nUnified Dense Reward Module for QAM\n\nSupports any cube manipulation environment (single, double, triple, etc.)\nby parameterizing on num_cubes and goal_positions, auto-detected from env_name.\n\nReward versions:\n- v1: Simple cube placement count\n- v2: Distance-based progress (recommended)\n- v3: Detailed stage tracking (reach -> grasp -> carry -> place per cube)\n- v4: Direct v2-delta (reward = v2(s') - v2(s) + gamma * success_bonus)\n- v5: Potential-based v3 (reward = \u03a6_v3(s') - \u03a6_v3(s))\n- v6: Pick-place style progress-delta (4 subtasks per cube) + terminal bonus\n- v7: Pick-place style progress-delta (3 subtasks, no release stage)\n      + placement event bonus/penalty + terminal bonus\n- v8: v7 + explicit release event bonus/penalty\n- v9: v8 without progress-potential shaping and without release event\n      (stage penalty + terminal bonus)\n- v10: v8-style reward with:\n       (1) continuous reach progress at threshold,\n       (2) hysteresis for reach->grasp switching,\n       (3) continuous lift progress at grasp threshold\n- v11: v10-style with extra lower stage:\n       reach -> grasp -> transport(XY) -> lower(Z). Lower activates only\n       inside XY success threshold and Z progress is normalized by the\n       entry-Z at XY-threshold crossing.\n- v12: v11 + wrong-XY penalty during lower stage:\n       lower-stage subprogress is in [-1, 1], starts at 0 on entry,\n       and becomes negative if XY distance increases from lower-stage entry.\n- v13: v11 + subtask transition event shaping (no release event):\n       +bonus for 1->2 and 2->3, same-magnitude penalty for 2->1 and 3->2.\n- v14: v13 with subtask transition event enabled.\n- v15: v12 + XY-worsening penalty during transport stage.\n- v16: v15 with release event removed.\n- v17: v16 + subtask transition event shaping.\n- v18: v15 + latched lower-entry-Z per active cube (first stage-3 entry only).\n- v20: v18 semantics with gripper-closure driven by physical left/right\n       fingertip gap (meters) when available.\n- v21: v18 semantics with the same physical gripper-gap and strict grasp\n       checks as v20.\n- v22: v21 semantics + monotonic stage/progress shaping (no reward for\n       re-achieving previously reached stages/progress after regressions).\n- v23: v8-style reward without release event\n       (stage penalty + shaping + terminal bonus).\n- v24: v23 + offline next-touch target-cube tracking.\n- v25: v24 + chunk-shaped online delivery.\n- v26: v25 reward semantics; intended for cube-consistent offline sequence sampling.\n- v27: v26 + pose-aligned reach and grasp-secure staging from offline behavior.\n- v28: v27 + explicit post-completion clear stage (release+raise+separate)\n       before switching to the next cube.\n- v29: explicit ordered placement exit:\n       place (at goal) -> release near table -> lift away -> switch cube.\n- v30: v29 + stricter stage-1 entry (aligned AND firm close).\n\"\"\"\n\nimport re\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\n\n# OGBench observation scaling constants\nOBS_XYZ_CENTER = np.array([0.425, 0.0, 0.0])\nOBS_XYZ_SCALER = 10.0\n\n# OGBench cube success criterion from\n# ogbench/manipspace/envs/cube_env.py::_compute_successes():\n# np.linalg.norm(obj_pos - tar_pos) <= 0.04\nCUBE_SUCCESS_THRESHOLD = 0.04\n\n# Approximate gripper home: center of OGBench arm_sampling_bounds\n# [[0.25, -0.35, 0.20], [0.6, 0.35, 0.35]]\nGRIPPER_HOME = np.array([0.425, 0.0, 0.275])\n# Tuned for OGBench cube task2 datasets (double/triple):\n# - gripper opening proxy is qpos[right_driver_joint] / 0.8 in [0, 1]\n# - higher means more closed; successful lifted non-stacked states cluster near ~0.56\nV6_REACH_THRESHOLD = 0.10\nV6_GOAL_APPROACH_THRESHOLD = 0.09\nV6_GRIPPER_CLOSED_THRESHOLD = 0.50\nV6_GRIPPER_CLOSED_REF = 0.56\nV6_GRASP_LIFT_THRESHOLD = 0.008\nV6_GRASP_LIFT_TARGET = 0.02\n# v10 reach hysteresis tuned from dataset contact-distance statistics.\nV10_REACH_THRESHOLD_IN = 0.10\nV10_REACH_THRESHOLD_OUT = 0.10\nV10_GRASP_LIFT_THRESHOLD = 0.015\nV10_MIN_CARRY_LIFT = 0.025\nV10_LOWER_XY_THRESHOLD = 0.04\nV10_XY_WORSEN_EPS = 0.002\nV10_LIFT_DROP_EPS = 0.003\nV10_FALLBACK_XY_AWAY_THRESHOLD = 0.03\nV10_CLOSE_BAND_MIN = 0.50\nV10_CLOSE_BAND_MAX = 0.60\nV10_CLOSE_DECAY_SIGMA = 0.10\nV10_CARRY_LIFT_MARGIN = 0.005\nV10_CARRY_STABLE_STEPS = 2\nV8_RELEASE_SUCCESS_BONUS = 0.5\nV8_RELEASE_FAR_PENALTY = 0.25\nV13_SUBTASK_TRANSITION_BONUS = 5.0\n# v20 gripper-gap normalization (meters):\n# close_score = clip((open_ref - gap) / (open_ref - close_ref), 0, 1)\n# with open_ref = max(initial_gap, 0.1) and close_ref fixed at 0.05.\nV20_GRIPPER_GAP_OPEN_FLOOR = 0.10\nV20_GRIPPER_GAP_CLOSE_REF = 0.055\nV20_PAD_CUBE_DIST_THRESHOLD = 0.045\nV20_PAD_CUBE_AVG_DIST_THRESHOLD = 0.04\nV21_REACH_PAD_CUBE_DIST_THRESHOLD = 0.07\nV23_STEP_PENALTY = 1.0\nV27_REACH_DIST_THRESHOLD = 0.07\nV27_REACH_XY_THRESHOLD = 0.03\nV27_REACH_Z_THRESHOLD = 0.02\nV27_GRASP_CLOSE_THRESHOLD = 0.50\nV27_GRASP_CLOSE_BASE = 0.45\nV27_GRASP_LIFT_CONFIRM = 0.02\nV28_RELEASE_OPEN_THRESHOLD = 0.45\nV28_PLACE_RELEASE_OPEN_THRESHOLD = 0.45\nV28_CLEAR_RAISE_ABOVE = 0.03\nV28_CLEAR_SEPARATE_DIST = 0.08\nV29_RELEASE_Z_MAX_ABOVE_INIT = 0.01\nV29_POST_RELEASE_LIFT_DELTA = 0.03\nV29_GRASP_RECENT_WINDOW = 8\nV30_STAGE1_CLOSE_THRESHOLD = 0.53\nV30_STAGE1_CLOSE_REF = 0.563\n\n\ndef compute_cube_order(init_positions: np.ndarray, reference: np.ndarray = GRIPPER_HOME) -> List[int]:\n    \"\"\"Compute fixed cube ordering by distance from reference position.\n\n    Returns a permutation of [0, ..., n-1] sorted by distance from reference\n    to each cube's initial position (nearest first).\n    \"\"\"\n    dists = [np.linalg.norm(reference - init_positions[i]) for i in range(len(init_positions))]\n    return list(np.argsort(dists))\n\n\ndef extract_gripper_pos(obs: np.ndarray) -> np.ndarray:\n    \"\"\"Extract end-effector position from OGBench observation.\n\n    obs[12:15] = (ee_pos - OBS_XYZ_CENTER) * OBS_XYZ_SCALER\n    \"\"\"\n    return obs[12:15] / OBS_XYZ_SCALER + OBS_XYZ_CENTER\n\n\ndef gripper_close_from_gap(gripper_gap_m: float, open_ref: float, close_ref: float = V20_GRIPPER_GAP_CLOSE_REF) -> float:\n    \"\"\"Map physical finger gap in meters to a close score in [0, 1].\"\"\"\n    denom = max(open_ref - close_ref, 1e-6)\n    return float(np.clip((open_ref - gripper_gap_m) / denom, 0.0, 1.0))\n\n\ndef extract_gripper_gap_from_sim(env_unwrapped) -> Optional[float]:\n    \"\"\"Best-effort left/right fingertip-gap extraction (meters) from MuJoCo state.\"\"\"\n    model = getattr(env_unwrapped, \"_model\", None) or getattr(env_unwrapped, \"model\", None)\n    data = getattr(env_unwrapped, \"_data\", None) or getattr(env_unwrapped, \"data\", None)\n    if model is None or data is None:\n        return None\n\n    cache = getattr(env_unwrapped, \"_qam_gripper_gap_pair\", None)\n    if cache is None:\n        try:\n            import mujoco\n        except Exception:\n            return None\n\n        def _name2id(obj_type, name):\n            try:\n                return int(mujoco.mj_name2id(model, obj_type, name))\n            except Exception:\n                return -1\n\n        # Prefer explicit site pairs if present; fall back to silicone-pad body centers.\n        site_pairs = [\n            (\"ur5e/robotiq/left_fingertip_site\", \"ur5e/robotiq/right_fingertip_site\"),\n            (\"ur5e/robotiq/left_fingertip\", \"ur5e/robotiq/right_fingertip\"),\n            (\"left_fingertip_site\", \"right_fingertip_site\"),\n        ]\n        for left_name, right_name in site_pairs:\n            left_id = _name2id(mujoco.mjtObj.mjOBJ_SITE, left_name)\n            right_id = _name2id(mujoco.mjtObj.mjOBJ_SITE, right_name)\n            if left_id >= 0 and right_id >= 0:\n                cache = (\"site\", left_id, right_id)\n                break\n\n        if cache is None:\n            body_pairs = [\n                (\"ur5e/robotiq/left_silicone_pad\", \"ur5e/robotiq/right_silicone_pad\"),\n                (\"ur5e/robotiq/left_pad\", \"ur5e/robotiq/right_pad\"),\n            ]\n            for left_name, right_name in body_pairs:\n                left_id = _name2id(mujoco.mjtObj.mjOBJ_BODY, left_name)\n                right_id = _name2id(mujoco.mjtObj.mjOBJ_BODY, right_name)\n                if left_id >= 0 and right_id >= 0:\n                    cache = (\"body\", left_id, right_id)\n                    break\n\n        setattr(env_unwrapped, \"_qam_gripper_gap_pair\", cache)\n\n    if cache is None:\n        return None\n\n    kind, left_id, right_id = cache\n    try:\n        if kind == \"site\":\n            left = data.site_xpos[left_id]\n            right = data.site_xpos[right_id]\n        else:\n            left = data.xpos[left_id]\n            right = data.xpos[right_id]\n        return float(np.linalg.norm(left - right))\n    except Exception:\n        return None\n\n\ndef extract_gripper_pad_positions_from_sim(env_unwrapped) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n    \"\"\"Best-effort extraction of left/right gripper pad world positions.\"\"\"\n    model = getattr(env_unwrapped, \"_model\", None) or getattr(env_unwrapped, \"model\", None)\n    data = getattr(env_unwrapped, \"_data\", None) or getattr(env_unwrapped, \"data\", None)\n    if model is None or data is None:\n        return None, None\n\n    cache = getattr(env_unwrapped, \"_qam_gripper_pad_pair\", None)\n    if cache is None:\n        try:\n            import mujoco\n        except Exception:\n            return None, None\n\n        def _name2id(obj_type, name):\n            try:\n                return int(mujoco.mj_name2id(model, obj_type, name))\n            except Exception:\n                return -1\n\n        site_pairs = [\n            (\"ur5e/robotiq/left_fingertip_site\", \"ur5e/robotiq/right_fingertip_site\"),\n            (\"ur5e/robotiq/left_fingertip\", \"ur5e/robotiq/right_fingertip\"),\n            (\"left_fingertip_site\", \"right_fingertip_site\"),\n        ]\n        for left_name, right_name in site_pairs:\n            left_id = _name2id(mujoco.mjtObj.mjOBJ_SITE, left_name)\n            right_id = _name2id(mujoco.mjtObj.mjOBJ_SITE, right_name)\n            if left_id >= 0 and right_id >= 0:\n                cache = (\"site\", left_id, right_id)\n                break\n\n        if cache is None:\n            body_pairs = [\n                (\"ur5e/robotiq/left_silicone_pad\", \"ur5e/robotiq/right_silicone_pad\"),\n                (\"ur5e/robotiq/left_pad\", \"ur5e/robotiq/right_pad\"),\n            ]\n            for left_name, right_name in body_pairs:\n                left_id = _name2id(mujoco.mjtObj.mjOBJ_BODY, left_name)\n                right_id = _name2id(mujoco.mjtObj.mjOBJ_BODY, right_name)\n                if left_id >= 0 and right_id >= 0:\n                    cache = (\"body\", left_id, right_id)\n                    break\n\n        setattr(env_unwrapped, \"_qam_gripper_pad_pair\", cache)\n\n    if cache is None:\n        return None, None\n\n    kind, left_id, right_id = cache\n    try:\n        if kind == \"site\":\n            return data.site_xpos[left_id].copy(), data.site_xpos[right_id].copy()\n        return data.xpos[left_id].copy(), data.xpos[right_id].copy()\n    except Exception:\n        return None, None\n\n\n# ============================================================\n# Task goal positions lookup: (env_type, task_id) -> goal_xyzs\n# From ogbench/ogbench/manipspace/envs/cube_env.py set_tasks()\n# ============================================================\nTASK_GOALS = {\n    # Single-play tasks (1 cube)\n    ('single', 1): np.array([[0.425, -0.1, 0.02]]),\n    ('single', 2): np.array([[0.50, 0.0, 0.02]]),\n    ('single', 3): np.array([[0.35, 0.0, 0.02]]),\n    ('single', 4): np.array([[0.50, 0.2, 0.02]]),\n    ('single', 5): np.array([[0.50, -0.2, 0.02]]),\n    # Double-play tasks (2 cubes)\n    ('double', 1): np.array([[0.425, 0.0, 0.02], [0.425, 0.1, 0.02]]),\n    ('double', 2): np.array([[0.35, 0.1, 0.02], [0.50, 0.1, 0.02]]),\n    ('double', 3): np.array([[0.425, -0.2, 0.02], [0.425, 0.2, 0.02]]),\n    ('double', 4): np.array([[0.425, 0.1, 0.02], [0.425, -0.1, 0.02]]),\n    ('double', 5): np.array([[0.425, 0.0, 0.02], [0.425, 0.0, 0.06]]),\n    # Triple-play tasks (3 cubes)\n    ('triple', 1): np.array([[0.35, -0.1, 0.02], [0.35, 0.1, 0.02], [0.50, 0.1, 0.02]]),\n    ('triple', 2): np.array([[0.50, 0.0, 0.02], [0.50, 0.2, 0.02], [0.50, -0.2, 0.02]]),\n    ('triple', 3): np.array([[0.35, -0.1, 0.02], [0.50, -0.2, 0.02], [0.50, 0.0, 0.02]]),\n    ('triple', 4): np.array([[0.50, -0.1, 0.02], [0.50, 0.1, 0.02], [0.35, 0.0, 0.02]]),\n    ('triple', 5): np.array([[0.425, 0.2, 0.02], [0.425, 0.2, 0.06], [0.425, 0.2, 0.10]]),\n}\n\n# Task initial positions: used by V2 for distance-based progress computation.\n# Without these, initial_position == current_position and progress_to_goal() is always 0.\nTASK_INITS = {\n    # Single-play tasks\n    ('single', 1): np.array([[0.425, 0.1, 0.02]]),\n    ('single', 2): np.array([[0.35, 0.0, 0.02]]),\n    ('single', 3): np.array([[0.50, 0.0, 0.02]]),\n    ('single', 4): np.array([[0.35, -0.2, 0.02]]),\n    ('single', 5): np.array([[0.35, 0.2, 0.02]]),\n    # Double-play tasks\n    ('double', 1): np.array([[0.425, 0.0, 0.02], [0.425, -0.1, 0.02]]),\n    ('double', 2): np.array([[0.35, -0.1, 0.02], [0.50, -0.1, 0.02]]),\n    ('double', 3): np.array([[0.35, 0.0, 0.02], [0.50, 0.0, 0.02]]),\n    ('double', 4): np.array([[0.425, -0.1, 0.02], [0.425, 0.1, 0.02]]),\n    ('double', 5): np.array([[0.425, -0.2, 0.02], [0.425, 0.2, 0.02]]),\n    # Triple-play tasks\n    ('triple', 1): np.array([[0.35, -0.1, 0.02], [0.35, 0.1, 0.02], [0.50, -0.1, 0.02]]),\n    ('triple', 2): np.array([[0.35, -0.2, 0.02], [0.35, 0.0, 0.02], [0.35, 0.2, 0.02]]),\n    ('triple', 3): np.array([[0.425, 0.2, 0.02], [0.425, 0.2, 0.06], [0.425, 0.2, 0.10]]),\n    ('triple', 4): np.array([[0.35, 0.0, 0.02], [0.50, -0.1, 0.02], [0.50, 0.1, 0.02]]),\n    ('triple', 5): np.array([[0.35, -0.1, 0.02], [0.50, -0.2, 0.02], [0.50, 0.0, 0.02]]),\n}\n\nENV_TYPE_NUM_CUBES = {\n    'single': 1,\n    'double': 2,\n    'triple': 3,\n    'quadruple': 4,\n    'octuple': 8,\n}\n\n\ndef parse_env_name(env_name: str) -> Tuple[str, int, int]:\n    \"\"\"Parse env_name to extract env_type, task_id, and num_cubes.\n\n    Examples:\n        'cube-single-play-singletask-task4-v0' -> ('single', 4, 1)\n        'cube-triple-play-singletask-task2-v0' -> ('triple', 2, 3)\n    \"\"\"\n    # Split on '-' and match against known env types as whole tokens\n    # to avoid 'single' matching inside 'singletask'\n    splits = env_name.split('-')\n    env_type = None\n    for token in splits:\n        if token in ENV_TYPE_NUM_CUBES:\n            env_type = token\n            break\n    if env_type is None:\n        raise ValueError(f\"Cannot determine env_type from: {env_name}\")\n\n    task_match = re.search(r'task(\\d+)', env_name)\n    task_id = int(task_match.group(1)) if task_match else 2  # default task\n\n    num_cubes = ENV_TYPE_NUM_CUBES[env_type]\n    return env_type, task_id, num_cubes\n\n\n# ============================================================\n# Generic Cube state model\n# ============================================================\nclass CubeState:\n    \"\"\"State for a single cube.\"\"\"\n    def __init__(self, position=None, goal_position=None, success_threshold=CUBE_SUCCESS_THRESHOLD):\n        self.position: np.ndarray = np.array(position) if position is not None else np.zeros(3)\n        self.initial_position: np.ndarray = self.position.copy()\n        self.goal_position: np.ndarray = np.array(goal_position) if goal_position is not None else self.position.copy()\n        self.success_threshold: float = success_threshold\n\n    def distance_to_goal(self) -> float:\n        return float(np.linalg.norm(self.position - self.goal_position))\n\n    def is_at_goal(self, threshold=None) -> bool:\n        if threshold is None:\n            threshold = self.success_threshold\n        return self.distance_to_goal() <= threshold\n\n    def progress_to_goal(self, threshold=None) -> float:\n        \"\"\"Progress from initial position to goal (0.0 to 1.0), saturating at success threshold.\"\"\"\n        if threshold is None:\n            threshold = self.success_threshold\n        initial_dist = np.linalg.norm(self.initial_position - self.goal_position)\n        current_dist = self.distance_to_goal()\n        if initial_dist < 1e-6:\n            return 1.0\n\n        # Match environment success semantics: once within threshold, progress is complete.\n        effective_current = max(current_dist - threshold, 0.0)\n        effective_initial = max(initial_dist - threshold, 1e-6)\n        return float(np.clip(1.0 - (effective_current / effective_initial), 0.0, 1.0))\n\n\nclass CubeEnvModel:\n    \"\"\"Lightweight physics-free environment model for reward computation.\"\"\"\n    def __init__(self, num_cubes: int, goal_positions: np.ndarray,\n                 init_positions: Optional[np.ndarray] = None,\n                 cube_order: Optional[List[int]] = None,\n                 success_threshold: float = CUBE_SUCCESS_THRESHOLD):\n        self.num_cubes = num_cubes\n        self.goal_positions = goal_positions\n        self.success_threshold = success_threshold\n        self.cubes: List[CubeState] = [\n            CubeState(goal_position=goal_positions[i], success_threshold=success_threshold) for i in range(num_cubes)\n        ]\n        # Set task initial positions for V2 distance-based progress\n        if init_positions is not None:\n            for i in range(num_cubes):\n                self.cubes[i].initial_position = init_positions[i].copy()\n        # Fixed cube ordering for v3/v5 sequential progress (nearest-first from gripper home)\n        self.cube_order: List[int] = cube_order if cube_order is not None else list(range(num_cubes))\n        # OGBench proxy in [0, 1]: 0=open, ~0.56=closed in these datasets.\n        self.gripper_width: float = 0.0\n        self.gripper_gap_m: Optional[float] = None\n        self.gripper_gap_open_ref_m: float = V20_GRIPPER_GAP_OPEN_FLOOR\n        self.left_gripper_pos: Optional[np.ndarray] = None\n        self.right_gripper_pos: Optional[np.ndarray] = None\n        self.gripper_pos: Optional[np.ndarray] = None\n        self.use_pad_reach: bool = False\n        self.pad_reach_threshold: float = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n        self.grasp_lift_threshold_override: Optional[float] = None\n        # Optional active-cube annotation used by offline reward relabeling.\n        self.active_cube_override: Optional[int] = None\n        # v28 post-completion clear-stage context.\n        self.clear_cube_idx: Optional[int] = None\n        self.clear_cube_z: Optional[float] = None\n        # v29 ordered place->release->lift context.\n        self.place_cube_idx: Optional[int] = None\n        self.place_cube_z: Optional[float] = None\n        self.release_ee_z: Optional[float] = None\n\n    def load_state(self, qpos: np.ndarray, qvel: np.ndarray = None,\n                   gripper_pos: Optional[np.ndarray] = None,\n                   gripper_gap_m: Optional[float] = None,\n                   gripper_gap_open_ref_m: Optional[float] = None,\n                   left_gripper_pos: Optional[np.ndarray] = None,\n                   right_gripper_pos: Optional[np.ndarray] = None):\n        \"\"\"Load state from qpos vector.\n\n        qpos layout: [arm_joints(6), gripper_joints(8), cube0_pos(3)+quat(4), cube1_pos(3)+quat(4), ...]\n        gripper_pos: end-effector position (from observation), needed for v3/v5 reach substage.\n        \"\"\"\n        for i in range(self.num_cubes):\n            pos_start = 14 + i * 7\n            self.cubes[i].position = qpos[pos_start:pos_start + 3].copy()\n        self.gripper_gap_m = None if gripper_gap_m is None else float(gripper_gap_m)\n        if gripper_gap_m is not None:\n            if gripper_gap_open_ref_m is None:\n                gripper_gap_open_ref_m = V20_GRIPPER_GAP_OPEN_FLOOR\n            self.gripper_gap_open_ref_m = float(max(gripper_gap_open_ref_m, V20_GRIPPER_GAP_OPEN_FLOOR))\n            self.gripper_width = gripper_close_from_gap(float(gripper_gap_m), open_ref=self.gripper_gap_open_ref_m)\n        else:\n            self.gripper_width = float(np.clip(qpos[6] / 0.8, 0.0, 1.0))\n            self.gripper_gap_open_ref_m = V20_GRIPPER_GAP_OPEN_FLOOR\n        if gripper_pos is not None:\n            self.gripper_pos = gripper_pos.copy()\n        self.left_gripper_pos = left_gripper_pos.copy() if left_gripper_pos is not None else None\n        self.right_gripper_pos = right_gripper_pos.copy() if right_gripper_pos is not None else None\n\n    def is_grasped(self, cube_idx: int) -> bool:\n        cube = self.cubes[cube_idx]\n        gripper_closed = self.gripper_width >= V6_GRIPPER_CLOSED_THRESHOLD\n        if self.gripper_gap_m is not None and self.left_gripper_pos is not None and self.right_gripper_pos is not None:\n            left_dist = float(np.linalg.norm(self.left_gripper_pos - cube.position))\n            right_dist = float(np.linalg.norm(self.right_gripper_pos - cube.position))\n            avg_dist = 0.5 * (left_dist + right_dist)\n            gripper_closed = (\n                gripper_closed\n                and (left_dist <= V20_PAD_CUBE_DIST_THRESHOLD)\n                and (right_dist <= V20_PAD_CUBE_DIST_THRESHOLD)\n                and (avg_dist <= V20_PAD_CUBE_AVG_DIST_THRESHOLD)\n            )\n        is_lifted = cube.position[2] > cube.initial_position[2] + 0.01\n        return gripper_closed and is_lifted\n\n\n# ============================================================\n# V1: Simple cube placement count\n# ============================================================\ndef progress_v1(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"Progress = number of cubes at goal. Range: [0, num_cubes].\"\"\"\n    num_at_goal = sum(c.is_at_goal() for c in env.cubes)\n    return (float(num_at_goal), num_at_goal)\n\n\n# ============================================================\n# V2: Distance-based progress\n# ============================================================\ndef dist_based_progress(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"Progress = sum of per-cube distance-based progress.\n    Each cube contributes progress_to_goal() in [0, 1].\n    Range: [0, num_cubes]. Fully continuous, no gap.\n    \"\"\"\n    total = 0.0\n    completed = 0\n    for cube in env.cubes:\n        total += cube.progress_to_goal()\n        if cube.is_at_goal():\n            completed += 1\n    return (total, completed)\n\n\ndef progress_v2(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"V2 distance-based progress.\"\"\"\n    return dist_based_progress(env)\n\n\ndef progress_v4(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"V4 uses the same distance-based progress source as V2.\"\"\"\n    return dist_based_progress(env)\n\n\n# ============================================================\n# V3: Stage tracking (not grasped -> grasped per cube)\n# ============================================================\ndef progress_v3(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"Progress with per-cube stage tracking (reach -> place).\n\n    For cube order position k (0-indexed):\n      - Reach stage contributes [k, k + 0.5]\n      - Place stage contributes [k + 0.5, k + 1.0]\n\n    Total range: [0, num_cubes].\n    \"\"\"\n    n = env.num_cubes\n\n    # Find first incomplete cube in cube_order (nearest-first from gripper home)\n    active_idx = None\n    order_pos = 0\n    for pos, idx in enumerate(env.cube_order):\n        if not env.cubes[idx].is_at_goal():\n            active_idx = idx\n            order_pos = pos\n            break\n\n    if active_idx is None:\n        return (float(n), n)\n\n    active_cube = env.cubes[active_idx]\n    # 0: reach, 1: place\n    substage = 1 if env.is_grasped(active_idx) else 0\n\n    stage = order_pos * 2 + substage\n    base_progress = float(order_pos)\n\n    # NOTE: Per requested design, reach reference distance is computed from\n    # this cube's initial position to the FIRST cube's goal position.\n    first_goal = env.cubes[env.cube_order[0]].goal_position\n    reach_ref_dist = float(np.linalg.norm(active_cube.initial_position - first_goal))\n    reach_ref_dist = max(reach_ref_dist, 1e-6)\n\n    place_init_dist = float(np.linalg.norm(active_cube.initial_position - active_cube.goal_position))\n    current_dist = active_cube.distance_to_goal()\n\n    # Sub-progress within current substage using fixed references\n    subprogress = 0.0\n    if substage == 0:\n        # Reach: gripper approaching the cube.\n        if env.gripper_pos is not None:\n            gripper_dist = float(np.linalg.norm(env.gripper_pos - active_cube.position))\n            reach_progress = float(np.clip(1.0 - gripper_dist / reach_ref_dist, 0.0, 1.0))\n            subprogress = 0.5 * reach_progress\n    else:\n        # Place: progress moving cube from its initial location to goal.\n        effective_current = max(current_dist - env.success_threshold, 0.0)\n        effective_init = max(place_init_dist - env.success_threshold, 1e-6)\n        place_progress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n        subprogress = 0.5 + 0.5 * place_progress\n\n    total = base_progress + subprogress\n    return (total, stage)\n\n\ndef progress_v6(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"Pick-place style progress for sequential cube placement.\n\n    For the active cube (first incomplete in fixed cube_order), progress is split\n    into 4 subtasks, each worth 0.25:\n      0. move-to-object\n      1. grasp-object\n      2. move-to-goal\n      3. release-object\n\n    Total range: [0, num_cubes].\n    \"\"\"\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    for pos, idx in enumerate(env.cube_order):\n        if not env.cubes[idx].is_at_goal():\n            active_idx = idx\n            order_pos = pos\n            break\n\n    if active_idx is None:\n        return (float(n), n * 4 - 1)\n\n    cube = env.cubes[active_idx]\n    cube_goal_xy_dist = float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2]))\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    # Open-progress proxy for release stage: 1=open, 0=closed.\n    gripper_open = float(np.clip((V6_GRIPPER_CLOSED_REF - env.gripper_width) / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n    # Use v6-specific grasp condition consistent with OGBench opening proxy.\n    grasped = _is_grasped_strict(env, active_idx, cube_lift)\n\n    gripper_dist = None\n    if env.gripper_pos is not None:\n        gripper_dist = float(np.linalg.norm(env.gripper_pos - cube.position))\n\n    # Determine subtask index following pick-place semantics.\n    if gripper_dist is not None and (gripper_dist > V6_REACH_THRESHOLD) and (not grasped):\n        subtask_idx = 0  # move to object\n    elif not grasped:\n        subtask_idx = 1  # grasp object\n    elif cube_goal_dist > V6_GOAL_APPROACH_THRESHOLD:\n        subtask_idx = 2  # move to goal\n    else:\n        subtask_idx = 3  # release object\n\n    # Compute normalized sub-progress for the active subtask.\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        subprogress = float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n    elif subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n        subprogress = 0.6 * lift_progress + 0.4 * close_progress\n    elif subtask_idx == 2:\n        init_goal_dist = float(np.linalg.norm(cube.initial_position - cube.goal_position))\n        init_goal_dist = max(init_goal_dist, 1e-6)\n        subprogress = float(np.clip(1.0 - (cube_goal_dist / init_goal_dist), 0.0, 1.0))\n    else:\n        subprogress = gripper_open\n\n    main_progress = float(order_pos) + (float(subtask_idx) + float(np.clip(subprogress, 0.0, 1.0))) / 4.0\n    stage = order_pos * 4 + subtask_idx\n    return (main_progress, stage)\n\n\ndef progress_v7(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"Pick-place style progress with no release stage.\n\n    For the active cube (first incomplete in fixed cube_order), progress is split\n    into 3 subtasks:\n      0. move-to-object\n      1. grasp-object\n      2. move-to-goal\n\n    Total range: [0, 3 * num_cubes] in stage units.\n    \"\"\"\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    if env.active_cube_override is not None:\n        candidate = int(env.active_cube_override)\n        if 0 <= candidate < n and (not env.cubes[candidate].is_at_goal()):\n            active_idx = candidate\n            order_pos = int(sum(c.is_at_goal() for c in env.cubes))\n    if active_idx is None:\n        for pos, idx in enumerate(env.cube_order):\n            if not env.cubes[idx].is_at_goal():\n                active_idx = idx\n                order_pos = pos\n                break\n\n    if active_idx is None:\n        return (float(n * 3), n * 3)\n\n    cube = env.cubes[active_idx]\n    cube_goal_dist = cube.distance_to_goal()\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    grasped = _is_grasped_strict(env, active_idx, cube_lift)\n\n    gripper_dist = None\n    if env.gripper_pos is not None:\n        gripper_dist = float(np.linalg.norm(env.gripper_pos - cube.position))\n\n    if (not grasped) and (gripper_dist is not None) and (gripper_dist > V6_REACH_THRESHOLD):\n        subtask_idx = 0  # move to object\n    elif not grasped:\n        subtask_idx = 1  # grasp object\n    else:\n        subtask_idx = 2  # move to goal\n\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        subprogress = float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n    elif subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n        subprogress = 0.6 * lift_progress + 0.4 * close_progress\n    else:\n        init_goal_dist = float(np.linalg.norm(cube.initial_position - cube.goal_position))\n        effective_current = max(cube_goal_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_dist - env.success_threshold, 1e-6)\n        subprogress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n\n    main_progress = float(order_pos * 3) + float(subtask_idx) + float(np.clip(subprogress, 0.0, 1.0))\n    stage = order_pos * 3 + subtask_idx\n    return (main_progress, stage)\n\n\ndef progress_v27(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"v27 progress: pose-aligned reach -> grasp-secure -> move-to-goal.\"\"\"\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    if env.active_cube_override is not None:\n        candidate = int(env.active_cube_override)\n        if 0 <= candidate < n and (not env.cubes[candidate].is_at_goal()):\n            active_idx = candidate\n            order_pos = int(sum(c.is_at_goal() for c in env.cubes))\n    if active_idx is None:\n        for pos, idx in enumerate(env.cube_order):\n            if not env.cubes[idx].is_at_goal():\n                active_idx = idx\n                order_pos = pos\n                break\n\n    if active_idx is None:\n        return (float(n * 3), n * 3)\n\n    cube = env.cubes[active_idx]\n    cube_goal_dist = cube.distance_to_goal()\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n\n    gripper_dist = None\n    d_xy = None\n    d_z = None\n    if env.gripper_pos is not None:\n        delta = env.gripper_pos - cube.position\n        gripper_dist = float(np.linalg.norm(delta))\n        d_xy = float(np.linalg.norm(delta[:2]))\n        d_z = float(abs(delta[2]))\n\n    aligned = (\n        gripper_dist is not None\n        and d_xy is not None\n        and d_z is not None\n        and (gripper_dist <= V27_REACH_DIST_THRESHOLD)\n        and (d_xy <= V27_REACH_XY_THRESHOLD)\n        and (d_z <= V27_REACH_Z_THRESHOLD)\n    )\n    grasp_confirmed = (env.gripper_width >= V27_GRASP_CLOSE_THRESHOLD) and (cube_lift > V27_GRASP_LIFT_CONFIRM)\n    close_candidate = env.gripper_width >= V27_GRASP_CLOSE_BASE\n\n    if grasp_confirmed:\n        subtask_idx = 2\n    elif aligned or close_candidate:\n        subtask_idx = 1\n    else:\n        subtask_idx = 0\n\n    if subtask_idx == 0:\n        if gripper_dist is None or d_xy is None or d_z is None:\n            subprogress = 0.0\n        else:\n            dist_prog = float(np.clip(1.0 - (gripper_dist / V27_REACH_DIST_THRESHOLD), 0.0, 1.0))\n            xy_prog = float(np.clip(1.0 - (d_xy / V27_REACH_XY_THRESHOLD), 0.0, 1.0))\n            z_prog = float(np.clip(1.0 - (d_z / V27_REACH_Z_THRESHOLD), 0.0, 1.0))\n            subprogress = 0.4 * dist_prog + 0.4 * xy_prog + 0.2 * z_prog\n    elif subtask_idx == 1:\n        close_denom = max(V27_GRASP_CLOSE_THRESHOLD - V27_GRASP_CLOSE_BASE, 1e-6)\n        close_prog = float(np.clip((env.gripper_width - V27_GRASP_CLOSE_BASE) / close_denom, 0.0, 1.0))\n        lift_prog = float(np.clip(cube_lift / V27_GRASP_LIFT_CONFIRM, 0.0, 1.0))\n        align_prog = 1.0 if aligned else 0.0\n        subprogress = 0.5 * close_prog + 0.3 * lift_prog + 0.2 * align_prog\n    else:\n        init_goal_dist = float(np.linalg.norm(cube.initial_position - cube.goal_position))\n        effective_current = max(cube_goal_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_dist - env.success_threshold, 1e-6)\n        subprogress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n\n    main_progress = float(order_pos * 3) + float(subtask_idx) + float(np.clip(subprogress, 0.0, 1.0))\n    stage = order_pos * 3 + subtask_idx\n    return (main_progress, stage)\n\n\ndef _v28_clear_done(env: CubeEnvModel, cube_idx: int, cube_z_ref: Optional[float]) -> bool:\n    if env.gripper_pos is None or cube_idx < 0 or cube_idx >= env.num_cubes:\n        return False\n    cube = env.cubes[cube_idx]\n    if not cube.is_at_goal():\n        return False\n    z_ref = float(cube.position[2]) if cube_z_ref is None else float(cube_z_ref)\n    d_done = float(np.linalg.norm(env.gripper_pos - cube.position))\n    released = float(env.gripper_width) <= V28_RELEASE_OPEN_THRESHOLD\n    raised = float(env.gripper_pos[2]) >= (z_ref + V28_CLEAR_RAISE_ABOVE)\n    separated = d_done >= V28_CLEAR_SEPARATE_DIST\n    return bool(released and raised and separated)\n\n\ndef _v28_is_place_release_completion(prev_env: CubeEnvModel, curr_env: CubeEnvModel, cube_idx: int) -> bool:\n    \"\"\"Completion gate for v28: cube reaches goal and gripper is explicitly released.\"\"\"\n    if cube_idx < 0 or cube_idx >= curr_env.num_cubes:\n        return False\n    prev_goal = bool(prev_env.cubes[cube_idx].is_at_goal())\n    curr_goal = bool(curr_env.cubes[cube_idx].is_at_goal())\n    released = float(curr_env.gripper_width) <= V28_PLACE_RELEASE_OPEN_THRESHOLD\n    return (not prev_goal) and curr_goal and released\n\n\ndef progress_v28(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"v28 progress: v27 + explicit post-completion clear stage.\"\"\"\n    n = env.num_cubes\n\n    clear_idx = env.clear_cube_idx\n    if clear_idx is not None:\n        clear_idx = int(clear_idx)\n        if 0 <= clear_idx < n and env.cubes[clear_idx].is_at_goal():\n            completed = int(sum(c.is_at_goal() for c in env.cubes))\n            order_pos = max(completed - 1, 0)\n            cube = env.cubes[clear_idx]\n            z_ref = float(cube.position[2]) if env.clear_cube_z is None else float(env.clear_cube_z)\n\n            if env.gripper_pos is None:\n                subprogress = 0.0\n            else:\n                d_done = float(np.linalg.norm(env.gripper_pos - cube.position))\n                open_prog = float(np.clip((V28_RELEASE_OPEN_THRESHOLD - float(env.gripper_width)) / V28_RELEASE_OPEN_THRESHOLD, 0.0, 1.0))\n                raise_prog = float(np.clip((float(env.gripper_pos[2]) - z_ref) / V28_CLEAR_RAISE_ABOVE, 0.0, 1.0))\n                sep_prog = float(np.clip(d_done / V28_CLEAR_SEPARATE_DIST, 0.0, 1.0))\n                subprogress = 0.4 * open_prog + 0.4 * raise_prog + 0.2 * sep_prog\n\n            main_progress = float(order_pos * 4) + 3.0 + float(np.clip(subprogress, 0.0, 1.0))\n            stage = order_pos * 4 + 3\n            return (main_progress, stage)\n\n    # Fall back to v27 active-cube staging, but mapped to 4-substage scale.\n    p27, stage27 = progress_v27(env)\n    order_pos = int(stage27 // 3)\n    subtask27 = int(stage27 % 3)\n    subprog = float(np.clip(p27 - float(stage27), 0.0, 1.0))\n    p28 = float(order_pos * 4 + subtask27 + subprog)\n    stage28 = order_pos * 4 + subtask27\n    return p28, stage28\n\n\ndef _v29_release_condition(env: CubeEnvModel, cube_idx: int) -> bool:\n    if cube_idx < 0 or cube_idx >= env.num_cubes:\n        return False\n    cube = env.cubes[cube_idx]\n    if not cube.is_at_goal():\n        return False\n    released = float(env.gripper_width) <= V28_RELEASE_OPEN_THRESHOLD\n    z_above_init = float(cube.position[2] - cube.initial_position[2])\n    near_table = z_above_init <= V29_RELEASE_Z_MAX_ABOVE_INIT\n    return bool(released and near_table)\n\n\ndef _v29_lift_done(env: CubeEnvModel, release_ee_z: Optional[float]) -> bool:\n    if release_ee_z is None or env.gripper_pos is None:\n        return False\n    released = float(env.gripper_width) <= V28_RELEASE_OPEN_THRESHOLD\n    lifted = float(env.gripper_pos[2]) >= (float(release_ee_z) + V29_POST_RELEASE_LIFT_DELTA)\n    return bool(released and lifted)\n\n\ndef _v29_is_grasp_confirmed(env: CubeEnvModel, cube_idx: int) -> bool:\n    if cube_idx < 0 or cube_idx >= env.num_cubes:\n        return False\n    cube = env.cubes[cube_idx]\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    return bool(_is_grasped_strict(env, cube_idx, cube_lift))\n\n\ndef _v29_update_recent_grasp_steps(env: CubeEnvModel, recent_steps: np.ndarray) -> None:\n    for c in range(env.num_cubes):\n        if _v29_is_grasp_confirmed(env, c):\n            recent_steps[c] = V29_GRASP_RECENT_WINDOW\n        else:\n            recent_steps[c] = max(int(recent_steps[c]) - 1, 0)\n\n\ndef _v29_cap_to_preplace(env: CubeEnvModel, cube_idx: int, progress: float, stage: int) -> Tuple[float, int]:\n    if cube_idx < 0 or cube_idx >= env.num_cubes:\n        return float(progress), int(stage)\n    if not env.cubes[cube_idx].is_at_goal():\n        return float(progress), int(stage)\n    completed = int(sum(c.is_at_goal() for c in env.cubes))\n    order_pos = max(completed - 1, 0)\n    cap_stage = int(order_pos * 5 + 2)\n    cap_progress = float(cap_stage + 0.999)\n    return min(float(progress), cap_progress), min(int(stage), cap_stage)\n\n\ndef progress_v29(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"v29 progress: v27 base + explicit place->release->lift ordered stage.\"\"\"\n    n = env.num_cubes\n    place_idx = env.place_cube_idx\n    if place_idx is not None:\n        place_idx = int(place_idx)\n        if 0 <= place_idx < n and env.cubes[place_idx].is_at_goal():\n            completed = int(sum(c.is_at_goal() for c in env.cubes))\n            order_pos = max(completed - 1, 0)\n            cube = env.cubes[place_idx]\n            if env.release_ee_z is None:\n                open_prog = float(\n                    np.clip(\n                        (V28_RELEASE_OPEN_THRESHOLD - float(env.gripper_width)) / V28_RELEASE_OPEN_THRESHOLD,\n                        0.0,\n                        1.0,\n                    )\n                )\n                z_above_init = float(cube.position[2] - cube.initial_position[2])\n                z_prog = float(np.clip(1.0 - (z_above_init / V29_RELEASE_Z_MAX_ABOVE_INIT), 0.0, 1.0))\n                subprogress = 0.6 * open_prog + 0.4 * z_prog\n                main_progress = float(order_pos * 5) + 3.0 + float(np.clip(subprogress, 0.0, 1.0))\n                stage = order_pos * 5 + 3\n                return main_progress, stage\n\n            if env.gripper_pos is None:\n                lift_prog = 0.0\n            else:\n                lift_prog = float(\n                    np.clip(\n                        (float(env.gripper_pos[2]) - float(env.release_ee_z)) / V29_POST_RELEASE_LIFT_DELTA,\n                        0.0,\n                        1.0,\n                    )\n                )\n            open_hold = 1.0 if float(env.gripper_width) <= V28_RELEASE_OPEN_THRESHOLD else 0.0\n            subprogress = 0.8 * lift_prog + 0.2 * open_hold\n            main_progress = float(order_pos * 5) + 4.0 + float(np.clip(subprogress, 0.0, 1.0))\n            stage = order_pos * 5 + 4\n            return main_progress, stage\n\n    # No pending place context: use v27 base mapped to 5-substage scale.\n    p27, stage27 = progress_v27(env)\n    order_pos = int(stage27 // 3)\n    subtask27 = int(stage27 % 3)\n    subprog = float(np.clip(p27 - float(stage27), 0.0, 1.0))\n    p29 = float(order_pos * 5 + subtask27 + subprog)\n    stage29 = order_pos * 5 + subtask27\n    return p29, stage29\n\n\ndef progress_v30(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"v30 progress: v29 ordered place/release/lift + stricter stage-1 entry.\"\"\"\n    n = env.num_cubes\n    place_idx = env.place_cube_idx\n    if place_idx is not None:\n        place_idx = int(place_idx)\n        if 0 <= place_idx < n and env.cubes[place_idx].is_at_goal():\n            completed = int(sum(c.is_at_goal() for c in env.cubes))\n            order_pos = max(completed - 1, 0)\n            cube = env.cubes[place_idx]\n            if env.release_ee_z is None:\n                open_prog = float(\n                    np.clip(\n                        (V28_RELEASE_OPEN_THRESHOLD - float(env.gripper_width)) / V28_RELEASE_OPEN_THRESHOLD,\n                        0.0,\n                        1.0,\n                    )\n                )\n                z_above_init = float(cube.position[2] - cube.initial_position[2])\n                z_prog = float(np.clip(1.0 - (z_above_init / V29_RELEASE_Z_MAX_ABOVE_INIT), 0.0, 1.0))\n                subprogress = 0.6 * open_prog + 0.4 * z_prog\n                main_progress = float(order_pos * 5) + 3.0 + float(np.clip(subprogress, 0.0, 1.0))\n                stage = order_pos * 5 + 3\n                return main_progress, stage\n\n            if env.gripper_pos is None:\n                lift_prog = 0.0\n            else:\n                lift_prog = float(\n                    np.clip(\n                        (float(env.gripper_pos[2]) - float(env.release_ee_z)) / V29_POST_RELEASE_LIFT_DELTA,\n                        0.0,\n                        1.0,\n                    )\n                )\n            open_hold = 1.0 if float(env.gripper_width) <= V28_RELEASE_OPEN_THRESHOLD else 0.0\n            subprogress = 0.8 * lift_prog + 0.2 * open_hold\n            main_progress = float(order_pos * 5) + 4.0 + float(np.clip(subprogress, 0.0, 1.0))\n            stage = order_pos * 5 + 4\n            return main_progress, stage\n\n    # v30 base: strict stage-1 entry.\n    active_idx = None\n    order_pos = 0\n    if env.active_cube_override is not None:\n        candidate = int(env.active_cube_override)\n        if 0 <= candidate < n and (not env.cubes[candidate].is_at_goal()):\n            active_idx = candidate\n            order_pos = int(sum(c.is_at_goal() for c in env.cubes))\n    if active_idx is None:\n        for pos, idx in enumerate(env.cube_order):\n            if not env.cubes[idx].is_at_goal():\n                active_idx = idx\n                order_pos = pos\n                break\n\n    if active_idx is None:\n        return (float(n * 5), n * 5)\n\n    cube = env.cubes[active_idx]\n    cube_goal_dist = cube.distance_to_goal()\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n\n    gripper_dist = None\n    d_xy = None\n    d_z = None\n    if env.gripper_pos is not None:\n        delta = env.gripper_pos - cube.position\n        gripper_dist = float(np.linalg.norm(delta))\n        d_xy = float(np.linalg.norm(delta[:2]))\n        d_z = float(abs(delta[2]))\n\n    aligned = (\n        gripper_dist is not None\n        and d_xy is not None\n        and d_z is not None\n        and (gripper_dist <= V27_REACH_DIST_THRESHOLD)\n        and (d_xy <= V27_REACH_XY_THRESHOLD)\n        and (d_z <= V27_REACH_Z_THRESHOLD)\n    )\n    strict_close = float(env.gripper_width) >= V30_STAGE1_CLOSE_THRESHOLD\n    grasp_confirmed = (env.gripper_width >= V27_GRASP_CLOSE_THRESHOLD) and (cube_lift > V27_GRASP_LIFT_CONFIRM)\n\n    if grasp_confirmed:\n        subtask_idx = 2\n    elif aligned or strict_close:\n        subtask_idx = 1\n    else:\n        subtask_idx = 0\n\n    if subtask_idx == 0:\n        if gripper_dist is None or d_xy is None or d_z is None:\n            subprogress = 0.0\n        else:\n            dist_prog = float(np.clip(1.0 - (gripper_dist / V27_REACH_DIST_THRESHOLD), 0.0, 1.0))\n            xy_prog = float(np.clip(1.0 - (d_xy / V27_REACH_XY_THRESHOLD), 0.0, 1.0))\n            z_prog = float(np.clip(1.0 - (d_z / V27_REACH_Z_THRESHOLD), 0.0, 1.0))\n            subprogress = 0.4 * dist_prog + 0.4 * xy_prog + 0.2 * z_prog\n    elif subtask_idx == 1:\n        close_denom = max(V30_STAGE1_CLOSE_REF - V30_STAGE1_CLOSE_THRESHOLD, 1e-6)\n        close_prog = float(np.clip((env.gripper_width - V30_STAGE1_CLOSE_THRESHOLD) / close_denom, 0.0, 1.0))\n        lift_prog = float(np.clip(cube_lift / V27_GRASP_LIFT_CONFIRM, 0.0, 1.0))\n        align_prog = 1.0 if aligned else 0.0\n        subprogress = 0.5 * close_prog + 0.3 * lift_prog + 0.2 * align_prog\n    else:\n        init_goal_dist = float(np.linalg.norm(cube.initial_position - cube.goal_position))\n        effective_current = max(cube_goal_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_dist - env.success_threshold, 1e-6)\n        subprogress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n\n    p_base = float(order_pos * 3) + float(subtask_idx) + float(np.clip(subprogress, 0.0, 1.0))\n    stage_base = order_pos * 3 + subtask_idx\n    subprog = float(np.clip(p_base - float(stage_base), 0.0, 1.0))\n    p30 = float(order_pos * 5 + subtask_idx + subprog)\n    stage30 = order_pos * 5 + subtask_idx\n    return p30, stage30\n\n\ndef _v10_determine_current_subtask(\n    env: CubeEnvModel,\n    cube: CubeState,\n    prev_subtask: Optional[int],\n    gripper_dist: Optional[float],\n) -> Tuple[int, float, float]:\n    \"\"\"Determine v10 subtask index and key features.\"\"\"\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    cube_goal_xy_dist = float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2]))\n    # Match v8/v7 grasp condition exactly.\n    grasped = (env.gripper_width >= V6_GRIPPER_CLOSED_THRESHOLD) and (cube_lift > V6_GRASP_LIFT_THRESHOLD)\n\n    if grasped:\n        return 2, cube_lift, cube_goal_xy_dist\n    if gripper_dist is None:\n        return 1, cube_lift, cube_goal_xy_dist\n\n    # Match v8/v7 reach->grasp switching exactly.\n    return (0 if gripper_dist > V6_REACH_THRESHOLD else 1), cube_lift, cube_goal_xy_dist\n\n\ndef _v10_determine_progress_for_subtask(\n    env: CubeEnvModel,\n    cube: CubeState,\n    subtask_idx: int,\n    gripper_dist: Optional[float],\n    cube_lift: float,\n    cube_goal_xy_dist: float,\n) -> float:\n    \"\"\"Compute v10 subprogress (v8-equivalent shaping).\"\"\"\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        return float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n\n    if subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n        return 0.6 * lift_progress + 0.4 * close_progress\n\n    cube_goal_dist = float(np.linalg.norm(cube.position - cube.goal_position))\n    init_goal_dist = float(np.linalg.norm(cube.initial_position - cube.goal_position))\n    effective_current = max(cube_goal_dist - env.success_threshold, 0.0)\n    effective_init = max(init_goal_dist - env.success_threshold, 1e-6)\n    return float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n\n\ndef _v10_force_to_grasp(\n    progress_val: float,\n    stage_val: int,\n    env: CubeEnvModel,\n    cube: CubeState,\n) -> Tuple[float, int, int]:\n    \"\"\"Force local stage to grasp/lift (subtask 1).\"\"\"\n    order_pos_local = int(stage_val // 3)\n    forced_subtask_local = 1\n    forced_subprogress_local = _v10_grasp_subprogress(\n        env,\n        float(cube.position[2] - cube.initial_position[2]),\n    )\n\n    progress_val = float(\n        order_pos_local * 3\n        + float(forced_subtask_local)\n        + float(np.clip(forced_subprogress_local, 0.0, 1.0))\n    )\n    stage_val = order_pos_local * 3 + forced_subtask_local\n    return progress_val, stage_val, forced_subtask_local\n\n\ndef _v10_normalized_xy_distance(env: CubeEnvModel, cube: CubeState) -> float:\n    init_goal_xy_dist = float(np.linalg.norm(cube.initial_position[:2] - cube.goal_position[:2]))\n    effective_current_xy = max(\n        float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2])) - env.success_threshold,\n        0.0,\n    )\n    effective_init_xy = max(init_goal_xy_dist - env.success_threshold, 1e-6)\n    return float(effective_current_xy / effective_init_xy)\n\n\ndef _v10_transport_regressed(prev_env: CubeEnvModel, curr_env: CubeEnvModel, active_idx: int) -> bool:\n    prev_cube = prev_env.cubes[active_idx]\n    curr_cube = curr_env.cubes[active_idx]\n    prev_xy_dist = float(np.linalg.norm(prev_cube.position[:2] - prev_cube.goal_position[:2]))\n    curr_xy_dist = float(np.linalg.norm(curr_cube.position[:2] - curr_cube.goal_position[:2]))\n    prev_lift = float(prev_cube.position[2] - prev_cube.initial_position[2])\n    curr_lift = float(curr_cube.position[2] - curr_cube.initial_position[2])\n    xy_worsened = curr_xy_dist > (prev_xy_dist + V10_XY_WORSEN_EPS)\n    lowered_away = (\n        curr_lift < (prev_lift - V10_LIFT_DROP_EPS)\n        and curr_xy_dist > V10_FALLBACK_XY_AWAY_THRESHOLD\n    )\n    return bool(xy_worsened or lowered_away)\n\n\ndef _progress_v10(\n    env: CubeEnvModel,\n    prev_subtask: Optional[int] = None,\n    prev_env: Optional[CubeEnvModel] = None,\n    carry_ok: Optional[bool] = None,\n) -> Tuple[float, int, int]:\n    \"\"\"V10 progress, implemented as a self-contained v8/v7-equivalent path.\"\"\"\n    _ = (prev_subtask, prev_env, carry_ok)  # Keep signature compatible with prior v10 API.\n\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    for pos, idx in enumerate(env.cube_order):\n        if not env.cubes[idx].is_at_goal():\n            active_idx = idx\n            order_pos = pos\n            break\n\n    if active_idx is None:\n        done_stage = n * 3\n        return (float(done_stage), done_stage, done_stage)\n\n    cube = env.cubes[active_idx]\n    cube_goal_xy_dist = float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2]))\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    grasped = _is_grasped_strict(env, active_idx, cube_lift)\n\n    gripper_dist = None\n    if env.gripper_pos is not None:\n        gripper_dist = float(np.linalg.norm(env.gripper_pos - cube.position))\n\n    if (not grasped) and (gripper_dist is not None) and (gripper_dist > V6_REACH_THRESHOLD):\n        subtask_idx = 0  # move to object\n    elif not grasped:\n        subtask_idx = 1  # grasp object\n    else:\n        subtask_idx = 2  # move to goal\n\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        subprogress = float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n    elif subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n        subprogress = 0.6 * lift_progress + 0.4 * close_progress\n    else:\n        init_goal_xy_dist = float(np.linalg.norm(cube.initial_position[:2] - cube.goal_position[:2]))\n        effective_current = max(cube_goal_xy_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_xy_dist - env.success_threshold, 1e-6)\n        subprogress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n\n    main_progress = float(order_pos * 3) + float(subtask_idx) + float(np.clip(subprogress, 0.0, 1.0))\n    stage = order_pos * 3 + subtask_idx\n    return (main_progress, stage, subtask_idx)\n\n\ndef progress_v10(env: CubeEnvModel) -> Tuple[float, int]:\n    progress, stage, _ = _progress_v10(env, prev_subtask=None)\n    return progress, stage\n\n\ndef _progress_v11(\n    env: CubeEnvModel,\n    prev_subtask: Optional[int] = None,\n    prev_env: Optional[CubeEnvModel] = None,\n    lower_entry_z: Optional[float] = None,\n    lower_entry_cube_idx: Optional[int] = None,\n) -> Tuple[float, int, int, Optional[float], Optional[int]]:\n    \"\"\"V11 progress with explicit lower stage after XY success.\"\"\"\n    _ = prev_subtask\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    for pos, idx in enumerate(env.cube_order):\n        if not env.cubes[idx].is_at_goal():\n            active_idx = idx\n            order_pos = pos\n            break\n\n    if active_idx is None:\n        done_stage = n * 4\n        return (float(done_stage), done_stage, done_stage, None, None)\n\n    if lower_entry_cube_idx != active_idx:\n        lower_entry_z = None\n\n    cube = env.cubes[active_idx]\n    cube_goal_xy_dist = float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2]))\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    grasped = _is_grasped_strict(env, active_idx, cube_lift)\n\n    gripper_dist = None\n    if env.gripper_pos is not None:\n        gripper_dist = float(np.linalg.norm(env.gripper_pos - cube.position))\n    reached_for_grasp = _is_reached_for_grasp(env, active_idx, gripper_dist)\n\n    if (not grasped) and (not reached_for_grasp):\n        subtask_idx = 0\n    elif not grasped:\n        subtask_idx = 1\n    elif cube_goal_xy_dist > env.success_threshold:\n        subtask_idx = 2\n    else:\n        subtask_idx = 3\n\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        subprogress = float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n    elif subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n        subprogress = 0.6 * lift_progress + 0.4 * close_progress\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n    elif subtask_idx == 2:\n        init_goal_xy_dist = float(np.linalg.norm(cube.initial_position[:2] - cube.goal_position[:2]))\n        effective_current = max(cube_goal_xy_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_xy_dist - env.success_threshold, 1e-6)\n        subprogress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n    else:\n        if lower_entry_z is None:\n            if prev_env is not None:\n                prev_cube = prev_env.cubes[active_idx]\n                prev_xy = float(np.linalg.norm(prev_cube.position[:2] - prev_cube.goal_position[:2]))\n                if prev_xy > prev_env.success_threshold:\n                    lower_entry_z = float(prev_cube.position[2])\n                else:\n                    lower_entry_z = float(cube.position[2])\n            else:\n                lower_entry_z = float(cube.position[2])\n        lower_entry_cube_idx = active_idx\n\n        goal_z = float(cube.goal_position[2])\n        entry_above = max(float(lower_entry_z - goal_z), 1e-6)\n        current_above = max(float(cube.position[2] - goal_z), 0.0)\n        subprogress = float(np.clip(1.0 - (current_above / entry_above), 0.0, 1.0))\n\n    main_progress = float(order_pos * 4) + float(subtask_idx) + float(np.clip(subprogress, 0.0, 1.0))\n    stage = order_pos * 4 + subtask_idx\n    return (main_progress, stage, subtask_idx, lower_entry_z, lower_entry_cube_idx)\n\n\ndef progress_v11(env: CubeEnvModel) -> Tuple[float, int]:\n    progress, stage, _, _, _ = _progress_v11(env, prev_subtask=None)\n    return progress, stage\n\n\ndef _progress_v12(\n    env: CubeEnvModel,\n    prev_subtask: Optional[int] = None,\n    prev_env: Optional[CubeEnvModel] = None,\n    lower_entry_z: Optional[float] = None,\n    lower_entry_cube_idx: Optional[int] = None,\n    lower_entry_xy_dist: Optional[float] = None,\n) -> Tuple[float, int, int, Optional[float], Optional[int], Optional[float]]:\n    \"\"\"V12 progress: v11 with lower-stage XY-worsening penalty.\"\"\"\n    _ = prev_subtask\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    for pos, idx in enumerate(env.cube_order):\n        if not env.cubes[idx].is_at_goal():\n            active_idx = idx\n            order_pos = pos\n            break\n\n    if active_idx is None:\n        done_stage = n * 4\n        return (float(done_stage), done_stage, done_stage, None, None, None)\n\n    if lower_entry_cube_idx != active_idx:\n        lower_entry_z = None\n        lower_entry_xy_dist = None\n\n    cube = env.cubes[active_idx]\n    cube_goal_xy_dist = float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2]))\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    grasped = _is_grasped_strict(env, active_idx, cube_lift)\n\n    gripper_dist = None\n    if env.gripper_pos is not None:\n        gripper_dist = float(np.linalg.norm(env.gripper_pos - cube.position))\n    reached_for_grasp = _is_reached_for_grasp(env, active_idx, gripper_dist)\n\n    if (not grasped) and (not reached_for_grasp):\n        subtask_idx = 0\n    elif not grasped:\n        subtask_idx = 1\n    elif cube_goal_xy_dist > env.success_threshold:\n        subtask_idx = 2\n    else:\n        subtask_idx = 3\n\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        subprogress = float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n        lower_entry_xy_dist = None\n    elif subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n        subprogress = 0.6 * lift_progress + 0.4 * close_progress\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n        lower_entry_xy_dist = None\n    elif subtask_idx == 2:\n        init_goal_xy_dist = float(np.linalg.norm(cube.initial_position[:2] - cube.goal_position[:2]))\n        effective_current = max(cube_goal_xy_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_xy_dist - env.success_threshold, 1e-6)\n        subprogress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n        lower_entry_xy_dist = None\n    else:\n        if lower_entry_z is None:\n            if prev_env is not None:\n                prev_cube = prev_env.cubes[active_idx]\n                prev_xy = float(np.linalg.norm(prev_cube.position[:2] - prev_cube.goal_position[:2]))\n                if prev_xy > prev_env.success_threshold:\n                    lower_entry_z = float(prev_cube.position[2])\n                else:\n                    lower_entry_z = float(cube.position[2])\n            else:\n                lower_entry_z = float(cube.position[2])\n        if lower_entry_xy_dist is None:\n            # Entry point for lower stage starts from 0 subprogress.\n            lower_entry_xy_dist = cube_goal_xy_dist\n        lower_entry_cube_idx = active_idx\n\n        goal_z = float(cube.goal_position[2])\n        entry_above = max(float(lower_entry_z - goal_z), 1e-6)\n        current_above = max(float(cube.position[2] - goal_z), 0.0)\n        z_progress = float(np.clip(1.0 - (current_above / entry_above), 0.0, 1.0))\n\n        xy_increase = max(cube_goal_xy_dist - float(lower_entry_xy_dist), 0.0)\n        xy_penalty = float(xy_increase / max(env.success_threshold, 1e-6))\n        subprogress = float(np.clip(z_progress - xy_penalty, -1.0, 1.0))\n\n    main_progress = float(order_pos * 4) + float(subtask_idx) + float(np.clip(subprogress, -1.0, 1.0))\n    stage = order_pos * 4 + subtask_idx\n    return (main_progress, stage, subtask_idx, lower_entry_z, lower_entry_cube_idx, lower_entry_xy_dist)\n\n\ndef progress_v12(env: CubeEnvModel) -> Tuple[float, int]:\n    progress, stage, _, _, _, _ = _progress_v12(env, prev_subtask=None)\n    return progress, stage\n\n\ndef _progress_v15(\n    env: CubeEnvModel,\n    prev_subtask: Optional[int] = None,\n    prev_env: Optional[CubeEnvModel] = None,\n    lower_entry_z: Optional[float] = None,\n    lower_entry_cube_idx: Optional[int] = None,\n    lower_entry_xy_dist: Optional[float] = None,\n) -> Tuple[float, int, int, Optional[float], Optional[int], Optional[float]]:\n    \"\"\"V15 progress: v12 + transport-stage XY-worsening penalty (no lower-stage XY penalty).\"\"\"\n    _ = prev_subtask\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    for pos, idx in enumerate(env.cube_order):\n        if not env.cubes[idx].is_at_goal():\n            active_idx = idx\n            order_pos = pos\n            break\n\n    if active_idx is None:\n        done_stage = n * 4\n        return (float(done_stage), done_stage, done_stage, None, None, None)\n\n    if lower_entry_cube_idx != active_idx:\n        lower_entry_z = None\n        lower_entry_xy_dist = None\n\n    cube = env.cubes[active_idx]\n    cube_goal_xy_dist = float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2]))\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    grasped = _is_grasped_strict(env, active_idx, cube_lift)\n\n    gripper_dist = None\n    if env.gripper_pos is not None:\n        gripper_dist = float(np.linalg.norm(env.gripper_pos - cube.position))\n\n    if (not grasped) and (gripper_dist is not None) and (gripper_dist > V6_REACH_THRESHOLD):\n        subtask_idx = 0\n    elif not grasped:\n        subtask_idx = 1\n    elif cube_goal_xy_dist > env.success_threshold:\n        subtask_idx = 2\n    else:\n        subtask_idx = 3\n\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        subprogress = float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n        lower_entry_xy_dist = None\n    elif subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n        subprogress = 0.6 * lift_progress + 0.4 * close_progress\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n        lower_entry_xy_dist = None\n    elif subtask_idx == 2:\n        init_goal_xy_dist = float(np.linalg.norm(cube.initial_position[:2] - cube.goal_position[:2]))\n        effective_current = max(cube_goal_xy_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_xy_dist - env.success_threshold, 1e-6)\n        transport_progress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n        xy_worsen_penalty = 0.0\n        if prev_env is not None:\n            prev_cube = prev_env.cubes[active_idx]\n            prev_xy_dist = float(np.linalg.norm(prev_cube.position[:2] - prev_cube.goal_position[:2]))\n            xy_increase = max(cube_goal_xy_dist - prev_xy_dist, 0.0)\n            xy_worsen_penalty = float(xy_increase / effective_init)\n        subprogress = float(np.clip(transport_progress - xy_worsen_penalty, -1.0, 1.0))\n        lower_entry_z = None\n        lower_entry_cube_idx = None\n        lower_entry_xy_dist = None\n    else:\n        if lower_entry_z is None:\n            if prev_env is not None:\n                prev_cube = prev_env.cubes[active_idx]\n                prev_xy = float(np.linalg.norm(prev_cube.position[:2] - prev_cube.goal_position[:2]))\n                if prev_xy > prev_env.success_threshold:\n                    lower_entry_z = float(prev_cube.position[2])\n                else:\n                    lower_entry_z = float(cube.position[2])\n            else:\n                lower_entry_z = float(cube.position[2])\n        if lower_entry_xy_dist is None:\n            lower_entry_xy_dist = cube_goal_xy_dist\n        lower_entry_cube_idx = active_idx\n\n        goal_z = float(cube.goal_position[2])\n        entry_above = max(float(lower_entry_z - goal_z), 1e-6)\n        current_above = max(float(cube.position[2] - goal_z), 0.0)\n        subprogress = float(np.clip(1.0 - (current_above / entry_above), 0.0, 1.0))\n\n    main_progress = float(order_pos * 4) + float(subtask_idx) + float(np.clip(subprogress, -1.0, 1.0))\n    stage = order_pos * 4 + subtask_idx\n    return (main_progress, stage, subtask_idx, lower_entry_z, lower_entry_cube_idx, lower_entry_xy_dist)\n\n\ndef progress_v15(env: CubeEnvModel) -> Tuple[float, int]:\n    progress, stage, _, _, _, _ = _progress_v15(env, prev_subtask=None)\n    return progress, stage\n\n\ndef progress_v16(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"V16 shares the same progress definition as v15.\"\"\"\n    return progress_v15(env)\n\n\ndef progress_v17(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"V17 shares the same progress definition as v15.\"\"\"\n    return progress_v15(env)\n\n\ndef _progress_v18(\n    env: CubeEnvModel,\n    prev_subtask: Optional[int] = None,\n    prev_env: Optional[CubeEnvModel] = None,\n    lower_entry_z: Optional[float] = None,\n    lower_entry_cube_idx: Optional[int] = None,\n    lower_entry_xy_dist: Optional[float] = None,\n) -> Tuple[float, int, int, Optional[float], Optional[int], Optional[float]]:\n    \"\"\"V18 progress: v15 with latched lower-entry-Z per active cube.\"\"\"\n    _ = prev_subtask\n    n = env.num_cubes\n\n    active_idx = None\n    order_pos = 0\n    if env.active_cube_override is not None:\n        candidate = int(env.active_cube_override)\n        if 0 <= candidate < n and (not env.cubes[candidate].is_at_goal()):\n            active_idx = candidate\n            # Treat override as the current incomplete-cube slot.\n            order_pos = int(sum(c.is_at_goal() for c in env.cubes))\n    if active_idx is None:\n        for pos, idx in enumerate(env.cube_order):\n            if not env.cubes[idx].is_at_goal():\n                active_idx = idx\n                order_pos = pos\n                break\n\n    if active_idx is None:\n        done_stage = n * 4\n        return (float(done_stage), done_stage, done_stage, None, None, None)\n\n    # Reset latch only when active cube changes.\n    if lower_entry_cube_idx != active_idx:\n        lower_entry_z = None\n        lower_entry_xy_dist = None\n\n    cube = env.cubes[active_idx]\n    cube_goal_xy_dist = float(np.linalg.norm(cube.position[:2] - cube.goal_position[:2]))\n    cube_lift = float(cube.position[2] - cube.initial_position[2])\n    grasped = _is_grasped_strict(env, active_idx, cube_lift)\n\n    gripper_dist = None\n    if env.gripper_pos is not None:\n        gripper_dist = float(np.linalg.norm(env.gripper_pos - cube.position))\n    reached_for_grasp = _is_reached_for_grasp(env, active_idx, gripper_dist)\n\n    if (not grasped) and (not reached_for_grasp):\n        subtask_idx = 0\n    elif not grasped:\n        subtask_idx = 1\n    elif cube_goal_xy_dist > env.success_threshold:\n        subtask_idx = 2\n    else:\n        subtask_idx = 3\n\n    if subtask_idx == 0:\n        init_reach_dist = float(np.linalg.norm(cube.initial_position - GRIPPER_HOME))\n        init_reach_dist = max(init_reach_dist, 1e-6)\n        cur_reach_dist = gripper_dist if gripper_dist is not None else init_reach_dist\n        subprogress = float(np.clip(1.0 - (cur_reach_dist / init_reach_dist), 0.0, 1.0))\n    elif subtask_idx == 1:\n        lift_progress = float(np.clip(cube_lift / V6_GRASP_LIFT_TARGET, 0.0, 1.0))\n        if env.use_pad_reach:\n            # v21: remove gripper-closure component from grasp-stage subprogress.\n            subprogress = lift_progress\n        else:\n            close_progress = float(np.clip(env.gripper_width / V6_GRIPPER_CLOSED_REF, 0.0, 1.0))\n            subprogress = 0.6 * lift_progress + 0.4 * close_progress\n    elif subtask_idx == 2:\n        init_goal_xy_dist = float(np.linalg.norm(cube.initial_position[:2] - cube.goal_position[:2]))\n        effective_current = max(cube_goal_xy_dist - env.success_threshold, 0.0)\n        effective_init = max(init_goal_xy_dist - env.success_threshold, 1e-6)\n        transport_progress = float(np.clip(1.0 - (effective_current / effective_init), 0.0, 1.0))\n        xy_worsen_penalty = 0.0\n        if prev_env is not None:\n            prev_cube = prev_env.cubes[active_idx]\n            prev_xy_dist = float(np.linalg.norm(prev_cube.position[:2] - prev_cube.goal_position[:2]))\n            xy_increase = max(cube_goal_xy_dist - prev_xy_dist, 0.0)\n            xy_worsen_penalty = float(xy_increase / effective_init)\n        subprogress = float(np.clip(transport_progress - xy_worsen_penalty, -1.0, 1.0))\n    else:\n        # Latch only first time entering lower stage for this active cube.\n        if lower_entry_z is None:\n            if prev_env is not None:\n                prev_cube = prev_env.cubes[active_idx]\n                prev_xy = float(np.linalg.norm(prev_cube.position[:2] - prev_cube.goal_position[:2]))\n                if prev_xy > prev_env.success_threshold:\n                    lower_entry_z = float(prev_cube.position[2])\n                else:\n                    lower_entry_z = float(cube.position[2])\n            else:\n                lower_entry_z = float(cube.position[2])\n            lower_entry_xy_dist = cube_goal_xy_dist\n            lower_entry_cube_idx = active_idx\n\n        goal_z = float(cube.goal_position[2])\n        entry_above = max(float(lower_entry_z - goal_z), 1e-6)\n        current_above = max(float(cube.position[2] - goal_z), 0.0)\n        subprogress = float(np.clip(1.0 - (current_above / entry_above), 0.0, 1.0))\n\n    main_progress = float(order_pos * 4) + float(subtask_idx) + float(np.clip(subprogress, -1.0, 1.0))\n    stage = order_pos * 4 + subtask_idx\n    return (main_progress, stage, subtask_idx, lower_entry_z, lower_entry_cube_idx, lower_entry_xy_dist)\n\n\ndef progress_v18(env: CubeEnvModel) -> Tuple[float, int]:\n    progress, stage, _, _, _, _ = _progress_v18(env, prev_subtask=None)\n    return progress, stage\n\n\ndef progress_v13(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"V13 shares the same progress definition as v11.\"\"\"\n    return progress_v11(env)\n\n\ndef progress_v14(env: CubeEnvModel) -> Tuple[float, int]:\n    \"\"\"V14 shares the same progress definition as v11.\"\"\"\n    return progress_v11(env)\n\n\ndef _v10_grasp_subprogress(env: CubeEnvModel, cube_lift: float) -> float:\n    \"\"\"Subprogress for v10 grasp/lift stage.\"\"\"\n    lift_denom = max(V10_MIN_CARRY_LIFT - V10_GRASP_LIFT_THRESHOLD, 1e-6)\n    lift_eff = max(cube_lift - V10_GRASP_LIFT_THRESHOLD, 0.0)\n    lift_progress = float(np.clip(lift_eff / lift_denom, 0.0, 1.0))\n\n    # Close-progress profile:\n    # - inside [0.5, 0.6] -> 1.0\n    # - outside band -> exponential decay by distance to band\n    gw = float(env.gripper_width)\n    d = max(V10_CLOSE_BAND_MIN - gw, gw - V10_CLOSE_BAND_MAX, 0.0)\n    sigma = max(V10_CLOSE_DECAY_SIGMA, 1e-6)\n    close_progress = float(np.exp(-((d / sigma) ** 2)))\n\n    return 0.6 * lift_progress + 0.4 * close_progress\n\n\ndef _is_grasped_strict(env: CubeEnvModel, cube_idx: int, cube_lift: float) -> bool:\n    \"\"\"Base grasp test with stricter v20 pad-to-cube distance checks.\"\"\"\n    if env.grasp_lift_threshold_override is not None:\n        lift_threshold = float(env.grasp_lift_threshold_override)\n    else:\n        lift_threshold = V6_GRASP_LIFT_TARGET if env.use_pad_reach else V6_GRASP_LIFT_THRESHOLD\n    grasped = (env.gripper_width >= V6_GRIPPER_CLOSED_THRESHOLD) and (cube_lift > lift_threshold)\n    if not grasped:\n        return False\n    if env.gripper_gap_m is None:\n        return True\n    if env.left_gripper_pos is None or env.right_gripper_pos is None:\n        return True\n    cube = env.cubes[cube_idx]\n    left_dist = float(np.linalg.norm(env.left_gripper_pos - cube.position))\n    right_dist = float(np.linalg.norm(env.right_gripper_pos - cube.position))\n    avg_dist = 0.5 * (left_dist + right_dist)\n    return (\n        (left_dist <= V20_PAD_CUBE_DIST_THRESHOLD)\n        and (right_dist <= V20_PAD_CUBE_DIST_THRESHOLD)\n        and (avg_dist <= V20_PAD_CUBE_AVG_DIST_THRESHOLD)\n    )\n\n\ndef _is_reached_for_grasp(env: CubeEnvModel, cube_idx: int, gripper_dist: Optional[float]) -> bool:\n    \"\"\"Reach condition: for v21 use bilateral pad-to-cube thresholds, else fall back.\"\"\"\n    if env.use_pad_reach:\n        if env.left_gripper_pos is None or env.right_gripper_pos is None:\n            return False\n        cube = env.cubes[cube_idx]\n        left_dist = float(np.linalg.norm(env.left_gripper_pos - cube.position))\n        right_dist = float(np.linalg.norm(env.right_gripper_pos - cube.position))\n        return (left_dist <= env.pad_reach_threshold) and (right_dist <= env.pad_reach_threshold)\n    if gripper_dist is None:\n        return False\n    return gripper_dist <= V6_REACH_THRESHOLD\n\n\n# ============================================================\n# Unified DenseRewardWrapper\n# ============================================================\nPROGRESS_FNS = {\n    'v1': progress_v1,\n    'v2': progress_v2,\n    'v3': progress_v3,\n    'v4': progress_v4,  # Potential-based version of v2\n    'v5': progress_v3,  # Potential-based version of v3\n    'v6': progress_v6,  # Pick-place style delta shaping\n    'v7': progress_v7,  # Pick-place without release stage\n    'v8': progress_v7,  # v7 progress + explicit release event shaping\n    'v9': progress_v7,  # v8 structure without progress-potential shaping/event\n    'v10': progress_v10,\n    'v11': progress_v11,\n    'v12': progress_v12,\n    'v15': progress_v15,\n    'v16': progress_v16,\n    'v17': progress_v17,\n    'v18': progress_v18,\n    'v20': progress_v18,\n    'v21': progress_v18,\n    'v22': progress_v18,\n    'v23': progress_v7,\n    'v24': progress_v7,\n    'v25': progress_v7,\n    'v26': progress_v7,\n    'v27': progress_v27,\n    'v28': progress_v28,\n    'v29': progress_v29,\n    'v30': progress_v30,\n    'v13': progress_v13,\n    'v14': progress_v14,\n}\n\n\nclass DenseRewardWrapper:\n    \"\"\"Unified wrapper for dense reward computation.\n\n    Automatically detects num_cubes and goal_positions from env_name.\n\n    Usage:\n        wrapper = DenseRewardWrapper(\n            task_name=\"cube-single-play-singletask-task4-v0\",\n            version=\"v2\",\n        )\n        rewards = wrapper.compute_dataset_rewards(ds, discount=0.99, terminal_bonus=1.0)\n    \"\"\"\n\n    def __init__(\n        self,\n        task_name: str,\n        version: str = \"v2\",\n        debug: bool = False,\n        success_threshold: float = CUBE_SUCCESS_THRESHOLD,\n        v23_step_penalty: float = V23_STEP_PENALTY,\n    ):\n        self.task_name = task_name\n        self.version = version\n        self.debug = debug\n        self.success_threshold = success_threshold\n        self.v23_step_penalty = float(v23_step_penalty)\n\n        # Auto-detect from env_name\n        self.env_type, self.task_id, self.num_cubes = parse_env_name(task_name)\n\n        key = (self.env_type, self.task_id)\n        if key not in TASK_GOALS:\n            raise ValueError(\n                f\"No goal positions for ({self.env_type}, task{self.task_id}). \"\n                f\"Available: {list(TASK_GOALS.keys())}\"\n            )\n        self.goal_positions = TASK_GOALS[key]\n        self.init_positions = TASK_INITS.get(key)\n        # Per-episode init positions (set from actual reset state for online/eval).\n        # When set, these override static TASK_INITS.\n        self.episode_init_positions: Optional[np.ndarray] = None\n        self._v10_prev_subtask: Optional[int] = None\n        self._v10_carry_stable_steps: int = 0\n        self._v11_prev_subtask: Optional[int] = None\n        self._v11_lower_entry_z: Optional[float] = None\n        self._v11_lower_entry_cube_idx: Optional[int] = None\n        self._v12_prev_subtask: Optional[int] = None\n        self._v12_lower_entry_z: Optional[float] = None\n        self._v12_lower_entry_cube_idx: Optional[int] = None\n        self._v12_lower_entry_xy_dist: Optional[float] = None\n        self._v13_prev_subtask: Optional[int] = None\n        self._v13_lower_entry_z: Optional[float] = None\n        self._v13_lower_entry_cube_idx: Optional[int] = None\n        self._v14_prev_subtask: Optional[int] = None\n        self._v14_lower_entry_z: Optional[float] = None\n        self._v14_lower_entry_cube_idx: Optional[int] = None\n        self._v15_prev_subtask: Optional[int] = None\n        self._v15_lower_entry_z: Optional[float] = None\n        self._v15_lower_entry_cube_idx: Optional[int] = None\n        self._v15_lower_entry_xy_dist: Optional[float] = None\n        self._v16_prev_subtask: Optional[int] = None\n        self._v16_lower_entry_z: Optional[float] = None\n        self._v16_lower_entry_cube_idx: Optional[int] = None\n        self._v16_lower_entry_xy_dist: Optional[float] = None\n        self._v17_prev_subtask: Optional[int] = None\n        self._v17_lower_entry_z: Optional[float] = None\n        self._v17_lower_entry_cube_idx: Optional[int] = None\n        self._v17_lower_entry_xy_dist: Optional[float] = None\n        self._v18_prev_subtask: Optional[int] = None\n        self._v18_lower_entry_z: Optional[float] = None\n        self._v18_lower_entry_cube_idx: Optional[int] = None\n        self._v18_lower_entry_xy_dist: Optional[float] = None\n        self._v20_prev_subtask: Optional[int] = None\n        self._v20_lower_entry_z: Optional[float] = None\n        self._v20_lower_entry_cube_idx: Optional[int] = None\n        self._v20_lower_entry_xy_dist: Optional[float] = None\n        self._v20_gap_open_ref_m: float = V20_GRIPPER_GAP_OPEN_FLOOR\n        self._v22_prev_subtask: Optional[int] = None\n        self._v22_lower_entry_z: Optional[float] = None\n        self._v22_lower_entry_cube_idx: Optional[int] = None\n        self._v22_lower_entry_xy_dist: Optional[float] = None\n        self._v22_gap_open_ref_m: float = V20_GRIPPER_GAP_OPEN_FLOOR\n        self._v22_best_progress: Optional[float] = None\n        self._v22_best_stage: Optional[int] = None\n        self._v24_target_cube_idx: Optional[int] = None\n        self._v28_clear_pending: bool = False\n        self._v28_clear_cube_idx: Optional[int] = None\n        self._v28_clear_cube_z: Optional[float] = None\n        self._v29_place_pending: bool = False\n        self._v29_place_cube_idx: Optional[int] = None\n        self._v29_place_cube_z: Optional[float] = None\n        self._v29_release_ee_z: Optional[float] = None\n\n        # Precompute cube ordering for v3/v5/v6: nearest cube to gripper home goes first\n        if self.init_positions is not None:\n            self.cube_order = compute_cube_order(self.init_positions)\n        else:\n            self.cube_order = list(range(self.num_cubes))\n\n        if version not in ('v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30'):\n            raise ValueError(f\"Unknown version: {version}. Choose from 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30'\")\n\n        if debug:\n            print(f\"[DenseReward-{version}] env_type={self.env_type}, task_id={self.task_id}, \"\n                  f\"num_cubes={self.num_cubes}\")\n            print(f\"[DenseReward-{version}] goal_positions={self.goal_positions.tolist()}\")\n            print(f\"[DenseReward-{version}] cube_order={self.cube_order}\")\n\n    @property\n    def max_progress(self) -> float:\n        \"\"\"Maximum raw progress value for this version.\"\"\"\n        if self.version in ('v3', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30'):\n            return float(self.num_cubes)\n        return float(self.num_cubes)\n\n    def compute_potential(self, qpos: np.ndarray, qvel: Optional[np.ndarray] = None,\n                          gripper_pos: Optional[np.ndarray] = None,\n                          gripper_gap_m: Optional[float] = None,\n                          gripper_gap_open_ref_m: Optional[float] = None,\n                          left_gripper_pos: Optional[np.ndarray] = None,\n                          right_gripper_pos: Optional[np.ndarray] = None) -> float:\n        \"\"\"Compute shaping potential/value from progress.\n\n        - v2/v3 use raw progress P_v2(s)/P_v3(s), range [0, num_cubes].\n        - other versions keep centered form, range [-num_cubes, 0].\n        \"\"\"\n        progress, _ = self.compute_progress(\n            qpos,\n            qvel,\n            gripper_pos,\n            gripper_gap_m=gripper_gap_m,\n            gripper_gap_open_ref_m=gripper_gap_open_ref_m,\n            left_gripper_pos=left_gripper_pos,\n            right_gripper_pos=right_gripper_pos,\n        )\n        if self.version in ('v2', 'v3', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30'):\n            return float(progress)\n        scale = self.num_cubes / self.max_progress\n        return float(progress * scale - self.num_cubes)\n\n    def _make_env(self) -> CubeEnvModel:\n        init_positions = self.episode_init_positions if self.episode_init_positions is not None else self.init_positions\n        return CubeEnvModel(\n            self.num_cubes,\n            self.goal_positions,\n            init_positions,\n            self.cube_order,\n            success_threshold=self.success_threshold,\n        )\n\n    def set_episode_initial_positions_from_qpos(self, qpos: np.ndarray) -> None:\n        \"\"\"Capture cube initial positions from the actual episode reset state.\"\"\"\n        init_positions = []\n        for i in range(self.num_cubes):\n            pos_start = 14 + i * 7\n            init_positions.append(qpos[pos_start:pos_start + 3].copy())\n        self.episode_init_positions = np.array(init_positions, dtype=np.float64)\n        self._v10_prev_subtask = None\n        self._v10_carry_stable_steps = 0\n        self._v11_prev_subtask = None\n        self._v11_lower_entry_z = None\n        self._v11_lower_entry_cube_idx = None\n        self._v12_prev_subtask = None\n        self._v12_lower_entry_z = None\n        self._v12_lower_entry_cube_idx = None\n        self._v12_lower_entry_xy_dist = None\n        self._v13_prev_subtask = None\n        self._v13_lower_entry_z = None\n        self._v13_lower_entry_cube_idx = None\n        self._v14_prev_subtask = None\n        self._v14_lower_entry_z = None\n        self._v14_lower_entry_cube_idx = None\n        self._v15_prev_subtask = None\n        self._v15_lower_entry_z = None\n        self._v15_lower_entry_cube_idx = None\n        self._v15_lower_entry_xy_dist = None\n        self._v16_prev_subtask = None\n        self._v16_lower_entry_z = None\n        self._v16_lower_entry_cube_idx = None\n        self._v16_lower_entry_xy_dist = None\n        self._v17_prev_subtask = None\n        self._v17_lower_entry_z = None\n        self._v17_lower_entry_cube_idx = None\n        self._v17_lower_entry_xy_dist = None\n        self._v18_prev_subtask = None\n        self._v18_lower_entry_z = None\n        self._v18_lower_entry_cube_idx = None\n        self._v18_lower_entry_xy_dist = None\n        self._v20_prev_subtask = None\n        self._v20_lower_entry_z = None\n        self._v20_lower_entry_cube_idx = None\n        self._v20_lower_entry_xy_dist = None\n        self._v20_gap_open_ref_m = V20_GRIPPER_GAP_OPEN_FLOOR\n        self._v22_prev_subtask = None\n        self._v22_lower_entry_z = None\n        self._v22_lower_entry_cube_idx = None\n        self._v22_lower_entry_xy_dist = None\n        self._v22_gap_open_ref_m = V20_GRIPPER_GAP_OPEN_FLOOR\n        self._v22_best_progress = None\n        self._v22_best_stage = None\n        self._v24_target_cube_idx = None\n        self._v28_clear_pending = False\n        self._v28_clear_cube_idx = None\n        self._v28_clear_cube_z = None\n        self._v29_place_pending = False\n        self._v29_place_cube_idx = None\n        self._v29_place_cube_z = None\n        self._v29_release_ee_z = None\n\n    @staticmethod\n    def _monotonic_progress_stage(\n        prev_progress: float,\n        curr_progress: float,\n        prev_stage: int,\n        curr_stage: int,\n        best_progress: Optional[float],\n        best_stage: Optional[int],\n    ) -> Tuple[float, float, int, float, int]:\n        \"\"\"Compute monotonicized progress/stage for anti-cycling shaping.\"\"\"\n        effective_prev_progress = max(float(prev_progress), float(best_progress) if best_progress is not None else float(prev_progress))\n        effective_curr_progress = max(effective_prev_progress, float(curr_progress))\n        effective_prev_stage = max(int(prev_stage), int(best_stage) if best_stage is not None else int(prev_stage))\n        effective_curr_stage = max(effective_prev_stage, int(curr_stage))\n        return (\n            effective_prev_progress,\n            effective_curr_progress,\n            effective_curr_stage,\n            effective_curr_progress,\n            effective_curr_stage,\n        )\n\n    @staticmethod\n    def _stage_penalty_from_stage(stage: int, num_cubes: int, num_substages: int = 3) -> float:\n        total_stages = num_cubes * num_substages\n        if stage >= total_stages:\n            return 0.0\n        order_pos = int(stage // num_substages)\n        subtask_idx = int(stage % num_substages)\n        num_incomplete = num_cubes - order_pos\n        remaining_full_substages = (num_incomplete - 1) * num_substages\n        current_subtask_penalty = num_substages - subtask_idx\n        return float(-(remaining_full_substages + current_subtask_penalty))\n\n    def _compute_v10_progress_with_state(\n        self,\n        qpos: np.ndarray,\n        qvel: Optional[np.ndarray] = None,\n        ob: Optional[np.ndarray] = None,\n        prev_subtask: Optional[int] = None,\n        prev_qpos: Optional[np.ndarray] = None,\n        prev_qvel: Optional[np.ndarray] = None,\n        prev_ob: Optional[np.ndarray] = None,\n        use_stateful_carry_counter: bool = False,\n    ) -> Tuple[float, int, int]:\n        def _load_env(state_qpos, state_qvel=None, state_ob=None):\n            gp_local = extract_gripper_pos(state_ob) if state_ob is not None else None\n            env_local = self._make_env()\n            env_local.load_state(state_qpos, qvel=state_qvel, gripper_pos=gp_local)\n            return env_local\n\n        curr_env = _load_env(qpos, qvel, ob)\n        prev_env = _load_env(prev_qpos, prev_qvel, prev_ob) if prev_qpos is not None else None\n\n        return _progress_v10(\n            curr_env,\n            prev_subtask=prev_subtask,\n            prev_env=prev_env, \n            carry_ok=None,\n        )\n\n    @staticmethod\n    def _active_cube_idx(env: CubeEnvModel) -> Optional[int]:\n        for idx in env.cube_order:\n            if not env.cubes[idx].is_at_goal():\n                return idx\n        return None\n\n    def _v8_release_event(self, prev_qpos: np.ndarray, curr_qpos: np.ndarray, prev_ob=None, curr_ob=None) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, gripper_pos=curr_gp)\n        if self.version == \"v8\":\n            prev_env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n            curr_env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n\n        active_idx = self._active_cube_idx(prev_env)\n        if active_idx is None:\n            return 0.0\n\n        prev_cube = prev_env.cubes[active_idx]\n        curr_cube = curr_env.cubes[active_idx]\n        prev_lift = float(prev_cube.position[2] - prev_cube.initial_position[2])\n        curr_lift = float(curr_cube.position[2] - curr_cube.initial_position[2])\n        was_grasped = _is_grasped_strict(prev_env, active_idx, prev_lift)\n        is_grasped = _is_grasped_strict(curr_env, active_idx, curr_lift)\n        if not (was_grasped and not is_grasped):\n            return 0.0\n\n        release_dist = curr_env.cubes[active_idx].distance_to_goal()\n        if release_dist <= self.success_threshold:\n            return V8_RELEASE_SUCCESS_BONUS\n        return -V8_RELEASE_FAR_PENALTY\n\n    @staticmethod\n    def _v13_subtask_transition_event(prev_subtask: int, next_subtask: int) -> float:\n        if (prev_subtask, next_subtask) in ((1, 2), (2, 3)):\n            return V13_SUBTASK_TRANSITION_BONUS\n        if (prev_subtask, next_subtask) in ((2, 1), (3, 2)):\n            return -V13_SUBTASK_TRANSITION_BONUS\n        return 0.0\n\n    def _v7_v8_stage_penalty(self, qpos: np.ndarray, qvel: Optional[np.ndarray] = None, ob=None) -> float:\n        \"\"\"Stage-dependent step penalty for v7/v8, scaled by remaining cubes.\"\"\"\n        gp = extract_gripper_pos(ob) if ob is not None else None\n        env = self._make_env()\n        env.load_state(qpos, qvel=qvel, gripper_pos=gp)\n        if self.version in (\"v8\", \"v23\", \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"v29\", \"v30\"):\n            env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n        _, stage = progress_v7(env)\n        return self._stage_penalty_from_stage(stage=stage, num_cubes=env.num_cubes, num_substages=3)\n\n    def compute_progress(self, qpos: np.ndarray, qvel: Optional[np.ndarray] = None,\n                         gripper_pos: Optional[np.ndarray] = None,\n                         gripper_gap_m: Optional[float] = None,\n                         gripper_gap_open_ref_m: Optional[float] = None,\n                         left_gripper_pos: Optional[np.ndarray] = None,\n                         right_gripper_pos: Optional[np.ndarray] = None) -> Tuple[float, int]:\n        \"\"\"Compute progress for a single state.\"\"\"\n        env = self._make_env()\n        env.load_state(\n            qpos,\n            qvel,\n            gripper_pos,\n            gripper_gap_m=gripper_gap_m,\n            gripper_gap_open_ref_m=gripper_gap_open_ref_m,\n            left_gripper_pos=left_gripper_pos,\n            right_gripper_pos=right_gripper_pos,\n        )\n        env.use_pad_reach = self.version in (\"v21\", \"v22\")\n        env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n        if self.version in (\"v8\", \"v23\", \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"v29\", \"v30\"):\n            env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n        return PROGRESS_FNS[self.version](env)\n\n    def compute_progress_for_logging(\n        self,\n        qpos: np.ndarray,\n        ob: Optional[np.ndarray] = None,\n        gripper_gap_m: Optional[float] = None,\n        gripper_gap_open_ref_m: Optional[float] = None,\n        left_gripper_pos: Optional[np.ndarray] = None,\n        right_gripper_pos: Optional[np.ndarray] = None,\n    ) -> Tuple[float, int]:\n        \"\"\"Progress for visualization using current internal state (for stateful versions).\"\"\"\n        gp = extract_gripper_pos(ob) if ob is not None else None\n        env = self._make_env()\n        env.load_state(\n            qpos,\n            qvel=None,\n            gripper_pos=gp,\n            gripper_gap_m=gripper_gap_m,\n            gripper_gap_open_ref_m=gripper_gap_open_ref_m,\n            left_gripper_pos=left_gripper_pos,\n            right_gripper_pos=right_gripper_pos,\n        )\n        env.use_pad_reach = self.version in (\"v21\", \"v22\")\n        env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n        if self.version in (\"v8\", \"v23\", \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"v29\", \"v30\"):\n            env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n\n        # Keep logging progress aligned with reward-time target-cube tracking.\n        if self.version in (\"v24\", \"v25\", \"v26\", \"v27\"):\n            target = self._v24_target_cube_idx\n            if target is None or env.cubes[int(target)].is_at_goal():\n                target = self._first_incomplete_cube_idx(env)\n            touched = self._detect_touched_cube_idx(env, ob)\n            if touched is not None:\n                target = touched\n            if target is not None:\n                env.active_cube_override = int(target)\n            self._v24_target_cube_idx = target\n            p, s = PROGRESS_FNS[self.version](env)\n            return float(p), int(s)\n        if self.version == \"v28\":\n            if self._v28_clear_pending and self._v28_clear_cube_idx is not None:\n                env.clear_cube_idx = int(self._v28_clear_cube_idx)\n                env.clear_cube_z = self._v28_clear_cube_z\n            target = self._v24_target_cube_idx\n            if target is not None:\n                env.active_cube_override = int(target)\n            p, s = progress_v28(env)\n            return float(p), int(s)\n        if self.version == \"v29\":\n            if self._v29_place_pending and self._v29_place_cube_idx is not None:\n                env.place_cube_idx = int(self._v29_place_cube_idx)\n                env.place_cube_z = self._v29_place_cube_z\n                env.release_ee_z = self._v29_release_ee_z\n            target = self._v24_target_cube_idx\n            if target is not None:\n                env.active_cube_override = int(target)\n            p, s = progress_v29(env)\n            return float(p), int(s)\n        if self.version == \"v30\":\n            if self._v29_place_pending and self._v29_place_cube_idx is not None:\n                env.place_cube_idx = int(self._v29_place_cube_idx)\n                env.place_cube_z = self._v29_place_cube_z\n                env.release_ee_z = self._v29_release_ee_z\n            target = self._v24_target_cube_idx\n            if target is not None:\n                env.active_cube_override = int(target)\n            p, s = progress_v30(env)\n            return float(p), int(s)\n\n        if self.version == 'v11':\n            p, s, _, _, _ = _progress_v11(\n                env,\n                prev_subtask=self._v11_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v11_lower_entry_z,\n                lower_entry_cube_idx=self._v11_lower_entry_cube_idx,\n            )\n            return float(p), int(s)\n        if self.version == 'v12':\n            p, s, _, _, _, _ = _progress_v12(\n                env,\n                prev_subtask=self._v12_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v12_lower_entry_z,\n                lower_entry_cube_idx=self._v12_lower_entry_cube_idx,\n                lower_entry_xy_dist=self._v12_lower_entry_xy_dist,\n            )\n            return float(p), int(s)\n        if self.version == 'v13':\n            p, s, _, _, _ = _progress_v11(\n                env,\n                prev_subtask=self._v13_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v13_lower_entry_z,\n                lower_entry_cube_idx=self._v13_lower_entry_cube_idx,\n            )\n            return float(p), int(s)\n        if self.version == 'v14':\n            p, s, _, _, _ = _progress_v11(\n                env,\n                prev_subtask=self._v14_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v14_lower_entry_z,\n                lower_entry_cube_idx=self._v14_lower_entry_cube_idx,\n            )\n            return float(p), int(s)\n        if self.version == 'v15':\n            p, s, _, _, _, _ = _progress_v15(\n                env,\n                prev_subtask=self._v15_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v15_lower_entry_z,\n                lower_entry_cube_idx=self._v15_lower_entry_cube_idx,\n                lower_entry_xy_dist=self._v15_lower_entry_xy_dist,\n            )\n            return float(p), int(s)\n        if self.version == 'v16':\n            p, s, _, _, _, _ = _progress_v15(\n                env,\n                prev_subtask=self._v16_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v16_lower_entry_z,\n                lower_entry_cube_idx=self._v16_lower_entry_cube_idx,\n                lower_entry_xy_dist=self._v16_lower_entry_xy_dist,\n            )\n            return float(p), int(s)\n        if self.version == 'v17':\n            p, s, _, _, _, _ = _progress_v15(\n                env,\n                prev_subtask=self._v17_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v17_lower_entry_z,\n                lower_entry_cube_idx=self._v17_lower_entry_cube_idx,\n                lower_entry_xy_dist=self._v17_lower_entry_xy_dist,\n            )\n            return float(p), int(s)\n        if self.version == 'v18':\n            p, s, _, _, _, _ = _progress_v18(\n                env,\n                prev_subtask=self._v18_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v18_lower_entry_z,\n                lower_entry_cube_idx=self._v18_lower_entry_cube_idx,\n                lower_entry_xy_dist=self._v18_lower_entry_xy_dist,\n            )\n            return float(p), int(s)\n        if self.version in ('v20', 'v21'):\n            p, s, _, _, _, _ = _progress_v18(\n                env,\n                prev_subtask=self._v20_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v20_lower_entry_z,\n                lower_entry_cube_idx=self._v20_lower_entry_cube_idx,\n                lower_entry_xy_dist=self._v20_lower_entry_xy_dist,\n            )\n            return float(p), int(s)\n        if self.version in ('v22',):\n            p, s, _, _, _, _ = _progress_v18(\n                env,\n                prev_subtask=self._v22_prev_subtask,\n                prev_env=None,\n                lower_entry_z=self._v22_lower_entry_z,\n                lower_entry_cube_idx=self._v22_lower_entry_cube_idx,\n                lower_entry_xy_dist=self._v22_lower_entry_xy_dist,\n            )\n            return float(p), int(s)\n\n        return self.compute_progress(\n            qpos,\n            qvel=None,\n            gripper_pos=gp,\n            gripper_gap_m=gripper_gap_m,\n            gripper_gap_open_ref_m=gripper_gap_open_ref_m,\n            left_gripper_pos=left_gripper_pos,\n            right_gripper_pos=right_gripper_pos,\n        )\n\n    def _require_keys(self, ds: Dict[str, np.ndarray], keys: List[str], ctx: str):\n        missing = [k for k in keys if k not in ds]\n        if missing:\n            raise ValueError(f\"{ctx} requires keys: {missing}\")\n\n    @staticmethod\n    def _first_incomplete_cube_idx(env: CubeEnvModel) -> Optional[int]:\n        for idx in env.cube_order:\n            if not env.cubes[idx].is_at_goal():\n                return int(idx)\n        return None\n\n    def _detect_touched_cube_idx(\n        self,\n        env: CubeEnvModel,\n        ob: Optional[np.ndarray],\n        touch_threshold: float = V21_REACH_PAD_CUBE_DIST_THRESHOLD,\n    ) -> Optional[int]:\n        if ob is None:\n            return None\n        try:\n            gp = extract_gripper_pos(ob)\n        except Exception:\n            return None\n        best_idx = None\n        best_dist = float(\"inf\")\n        for idx in range(env.num_cubes):\n            if env.cubes[idx].is_at_goal():\n                continue\n            d = float(np.linalg.norm(gp - env.cubes[idx].position))\n            if d < best_dist:\n                best_dist = d\n                best_idx = idx\n        if best_idx is None or best_dist > touch_threshold:\n            return None\n        return int(best_idx)\n\n    def _infer_offline_next_touch_targets(\n        self,\n        qpos_data: np.ndarray,\n        obs_data: Optional[np.ndarray],\n        terminals_data: Optional[np.ndarray],\n        touch_threshold: float = V21_REACH_PAD_CUBE_DIST_THRESHOLD,\n    ) -> np.ndarray:\n        \"\"\"Infer per-step active cube labels from \"next touch\" events.\n\n        For each episode segment, we detect touch events using gripper-to-cube\n        distance in observation space and back-fill each earlier step with the\n        next touched cube index. Values are -1 when no target can be inferred.\n        \"\"\"\n        n_steps = len(qpos_data)\n        targets = np.full(n_steps, -1, dtype=np.int32)\n        if obs_data is None:\n            return targets\n\n        next_touch_idx: Optional[int] = None\n        for i in range(n_steps - 1, -1, -1):\n            touched_idx: Optional[int] = None\n            try:\n                gp = extract_gripper_pos(obs_data[i])\n            except Exception:\n                gp = None\n\n            if gp is not None:\n                best_idx = -1\n                best_dist = float(\"inf\")\n                for cube_idx in range(self.num_cubes):\n                    pos_start = 14 + cube_idx * 7\n                    cube_pos = qpos_data[i][pos_start:pos_start + 3]\n                    if float(np.linalg.norm(cube_pos - self.goal_positions[cube_idx])) <= self.success_threshold:\n                        continue\n                    d = float(np.linalg.norm(gp - cube_pos))\n                    if d < best_dist:\n                        best_dist = d\n                        best_idx = cube_idx\n                if best_idx >= 0 and best_dist <= touch_threshold:\n                    touched_idx = best_idx\n\n            if touched_idx is not None:\n                next_touch_idx = touched_idx\n\n            targets[i] = -1 if next_touch_idx is None else int(next_touch_idx)\n\n            # Prevent carrying labels across episode boundaries.\n            if terminals_data is not None and bool(terminals_data[i]):\n                next_touch_idx = None\n\n        return targets\n\n    def _is_success_state(self, qpos: np.ndarray, qvel: Optional[np.ndarray] = None) -> bool:\n        \"\"\"Task success from post-state only (all cubes at goal).\"\"\"\n        env = self._make_env()\n        env.load_state(qpos, qvel=qvel, gripper_pos=None)\n        return all(c.is_at_goal() for c in env.cubes)\n\n    def count_success_cubes(self, qpos: np.ndarray, qvel: Optional[np.ndarray] = None) -> int:\n        \"\"\"Count cubes currently within success threshold.\"\"\"\n        env = self._make_env()\n        env.load_state(qpos, qvel=qvel, gripper_pos=None)\n        return int(sum(c.is_at_goal() for c in env.cubes))\n\n    def _success_bonus_post(self, next_qpos: np.ndarray, next_qvel: Optional[np.ndarray], terminal_bonus: float) -> float:\n        \"\"\"Post-success bonus: bonus_t = beta * 1[success(s_{t+1})].\"\"\"\n        return terminal_bonus if self._is_success_state(next_qpos, qvel=next_qvel) else 0.0\n\n    def compute_v1_dataset_rewards(self, ds: Dict[str, np.ndarray], terminal_bonus: float = 50.0, **_) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos'], \"v1\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            out[i] = self.compute_potential(qpos_data[i], qvel) + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v2_dataset_rewards(self, ds: Dict[str, np.ndarray], terminal_bonus: float = 50.0, **_) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos'], \"v2\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            out[i] = self.compute_potential(qpos_data[i], qvel) + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v3_dataset_rewards(self, ds: Dict[str, np.ndarray], terminal_bonus: float = 50.0, **_) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos'], \"v3\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        obs_data = ds.get('observations', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            gp = extract_gripper_pos(obs_data[i]) if obs_data is not None else None\n            out[i] = self.compute_potential(qpos_data[i], qvel, gripper_pos=gp) + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v4_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v4\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        base_rewards = ds['rewards']\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            curr_progress, _ = self.compute_progress(qpos_data[i], qvel)\n            next_progress, _ = self.compute_progress(next_qpos_data[i], next_qvel)\n            shaping = shaping_coef * (discount * next_progress - curr_progress)\n            out[i] = base_rewards[i] + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v5_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v5\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        base_rewards = ds['rewards']\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            gp = extract_gripper_pos(obs_data[i]) if obs_data is not None else None\n            next_gp = extract_gripper_pos(next_obs_data[i]) if next_obs_data is not None else None\n            curr_progress, _ = self.compute_progress(qpos_data[i], qvel, gripper_pos=gp)\n            next_progress, _ = self.compute_progress(next_qpos_data[i], next_qvel, gripper_pos=next_gp)\n            shaping = shaping_coef * (discount * next_progress - curr_progress)\n            out[i] = base_rewards[i] + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v6_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v6\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        base_rewards = ds['rewards']\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            gp = extract_gripper_pos(obs_data[i]) if obs_data is not None else None\n            next_gp = extract_gripper_pos(next_obs_data[i]) if next_obs_data is not None else None\n            curr_progress, _ = self.compute_progress(qpos_data[i], qvel, gripper_pos=gp)\n            next_progress, _ = self.compute_progress(next_qpos_data[i], next_qvel, gripper_pos=next_gp)\n            shaping = shaping_coef * (discount * next_progress - curr_progress)\n            out[i] = base_rewards[i] + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v7_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v7\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            gp = extract_gripper_pos(obs_data[i]) if obs_data is not None else None\n            next_gp = extract_gripper_pos(next_obs_data[i]) if next_obs_data is not None else None\n            curr_progress, _ = self.compute_progress(qpos_data[i], qvel, gripper_pos=gp)\n            next_progress, _ = self.compute_progress(next_qpos_data[i], next_qvel, gripper_pos=next_gp)\n            shaping = shaping_coef * (discount * next_progress - curr_progress)\n            stage_penalty = self._v7_v8_stage_penalty(\n                qpos=next_qpos_data[i],\n                qvel=next_qvel,\n                ob=next_obs_data[i] if next_obs_data is not None else None,\n            )\n\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v8_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v8\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            gp = extract_gripper_pos(obs_data[i]) if obs_data is not None else None\n            next_gp = extract_gripper_pos(next_obs_data[i]) if next_obs_data is not None else None\n            curr_progress, _ = self.compute_progress(qpos_data[i], qvel, gripper_pos=gp)\n            next_progress, _ = self.compute_progress(next_qpos_data[i], next_qvel, gripper_pos=next_gp)\n            shaping = shaping_coef * (discount * next_progress - curr_progress)\n            stage_penalty = self._v7_v8_stage_penalty(\n                qpos=next_qpos_data[i],\n                qvel=next_qvel,\n                ob=next_obs_data[i] if next_obs_data is not None else None,\n            )\n            event = self._v8_release_event(\n                prev_qpos=qpos_data[i],\n                curr_qpos=next_qpos_data[i],\n                prev_ob=obs_data[i] if obs_data is not None else None,\n                curr_ob=next_obs_data[i] if next_obs_data is not None else None,\n            )\n            out[i] = stage_penalty + shaping + event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v9_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v9\")\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        next_obs_data = ds.get('next_observations', None)\n        out = np.zeros(len(next_qpos_data), dtype=np.float32)\n        for i in range(len(next_qpos_data)):\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            stage_penalty = self._v7_v8_stage_penalty(\n                qpos=next_qpos_data[i],\n                qvel=next_qvel,\n                ob=next_obs_data[i] if next_obs_data is not None else None,\n            )\n            out[i] = stage_penalty + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v10_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v10\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            gp = extract_gripper_pos(obs_data[i]) if obs_data is not None else None\n            next_gp = extract_gripper_pos(next_obs_data[i]) if next_obs_data is not None else None\n            curr_progress, _ = self.compute_progress(qpos_data[i], qvel, gripper_pos=gp)\n            next_progress, _ = self.compute_progress(next_qpos_data[i], next_qvel, gripper_pos=next_gp)\n            shaping = shaping_coef * (discount * next_progress - curr_progress)\n            stage_penalty = self._v7_v8_stage_penalty(\n                qpos=next_qpos_data[i],\n                qvel=next_qvel,\n                ob=next_obs_data[i] if next_obs_data is not None else None,\n            )\n            event = self._v8_release_event(\n                prev_qpos=qpos_data[i],\n                curr_qpos=next_qpos_data[i],\n                prev_ob=obs_data[i] if obs_data is not None else None,\n                curr_ob=next_obs_data[i] if next_obs_data is not None else None,\n            )\n            out[i] = stage_penalty + shaping + event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n        return out\n\n    def compute_v11_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v11\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev = _progress_v11(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next = _progress_v11(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            event = self._v8_release_event(\n                prev_qpos=qpos_data[i],\n                curr_qpos=next_qpos_data[i],\n                prev_ob=prev_ob_i,\n                curr_ob=next_ob_i,\n            )\n            out[i] = stage_penalty + shaping + event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n        return out\n\n    def compute_v12_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v12\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n        lower_entry_xy_dist_state: Optional[float] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v12(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n                lower_entry_xy_dist=lower_entry_xy_dist_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next, lower_entry_xy_next = _progress_v12(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n                lower_entry_xy_dist=lower_entry_xy_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            event = self._v8_release_event(\n                prev_qpos=qpos_data[i],\n                curr_qpos=next_qpos_data[i],\n                prev_ob=prev_ob_i,\n                curr_ob=next_ob_i,\n            )\n            out[i] = stage_penalty + shaping + event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            lower_entry_xy_dist_state = lower_entry_xy_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n                lower_entry_xy_dist_state = None\n        return out\n\n    def compute_v13_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v13\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev = _progress_v11(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next = _progress_v11(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            # transition_event = self._v13_subtask_transition_event(\n            #     prev_subtask=prev_subtask_resolved,\n            #     next_subtask=next_subtask,\n            # )\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n        return out\n\n    def compute_v15_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v15\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n        lower_entry_xy_dist_state: Optional[float] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v15(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n                lower_entry_xy_dist=lower_entry_xy_dist_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next, lower_entry_xy_next = _progress_v15(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n                lower_entry_xy_dist=lower_entry_xy_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            event = self._v8_release_event(\n                prev_qpos=qpos_data[i],\n                curr_qpos=next_qpos_data[i],\n                prev_ob=prev_ob_i,\n                curr_ob=next_ob_i,\n            )\n            out[i] = stage_penalty + shaping + event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            lower_entry_xy_dist_state = lower_entry_xy_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n                lower_entry_xy_dist_state = None\n        return out\n\n    def compute_v16_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v16\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n        lower_entry_xy_dist_state: Optional[float] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v15(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n                lower_entry_xy_dist=lower_entry_xy_dist_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next, lower_entry_xy_next = _progress_v15(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n                lower_entry_xy_dist=lower_entry_xy_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            lower_entry_xy_dist_state = lower_entry_xy_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n                lower_entry_xy_dist_state = None\n        return out\n\n    def compute_v17_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v17\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n        lower_entry_xy_dist_state: Optional[float] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v15(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n                lower_entry_xy_dist=lower_entry_xy_dist_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next, lower_entry_xy_next = _progress_v15(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n                lower_entry_xy_dist=lower_entry_xy_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            transition_event = self._v13_subtask_transition_event(\n                prev_subtask=prev_subtask_resolved,\n                next_subtask=next_subtask,\n            )\n            out[i] = stage_penalty + shaping + transition_event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            lower_entry_xy_dist_state = lower_entry_xy_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n                lower_entry_xy_dist_state = None\n        return out\n\n    def compute_v18_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v18\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n        lower_entry_xy_dist_state: Optional[float] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            prev_env.use_pad_reach = self.version in (\"v21\", \"v22\", \"v23\")\n            prev_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n            next_env.use_pad_reach = self.version in (\"v21\", \"v22\", \"v23\")\n            next_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v18(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n                lower_entry_xy_dist=lower_entry_xy_dist_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next, lower_entry_xy_next = _progress_v18(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n                lower_entry_xy_dist=lower_entry_xy_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            event = self._v8_release_event(\n                prev_qpos=qpos_data[i],\n                curr_qpos=next_qpos_data[i],\n                prev_ob=prev_ob_i,\n                curr_ob=next_ob_i,\n            )\n            out[i] = stage_penalty + shaping + event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            lower_entry_xy_dist_state = lower_entry_xy_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n                lower_entry_xy_dist_state = None\n        return out\n\n    def compute_v20_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V20 reuses v18 dataset path (no MuJoCo fingertip gap in offline NPZ).\"\"\"\n        return self.compute_v18_dataset_rewards(\n            ds=ds,\n            discount=discount,\n            terminal_bonus=terminal_bonus,\n            shaping_coef=shaping_coef,\n        )\n\n    def compute_v22_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        step_penalty: float = 0.0,\n    ) -> np.ndarray:\n        \"\"\"V22 dataset rewards: v21 semantics with monotonic anti-cycling.\"\"\"\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v22\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n        target_cube_labels = self._infer_offline_next_touch_targets(\n            qpos_data=qpos_data,\n            obs_data=obs_data,\n            terminals_data=terminals_data,\n        )\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n        lower_entry_xy_dist_state: Optional[float] = None\n        best_progress_state: Optional[float] = None\n        best_stage_state: Optional[int] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            prev_env.use_pad_reach = self.version in (\"v21\", \"v22\", \"v23\")\n            prev_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n            next_env.use_pad_reach = self.version in (\"v21\", \"v22\", \"v23\")\n            next_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n            tracked_cube_idx = int(target_cube_labels[i])\n            if tracked_cube_idx >= 0:\n                prev_env.active_cube_override = tracked_cube_idx\n                next_env.active_cube_override = tracked_cube_idx\n\n            prev_progress, prev_stage, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v18(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n                lower_entry_xy_dist=lower_entry_xy_dist_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next, lower_entry_xy_next = _progress_v18(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n                lower_entry_xy_dist=lower_entry_xy_prev,\n            )\n            mono_prev_progress, mono_next_progress, mono_next_stage, best_progress_next, best_stage_next = self._monotonic_progress_stage(\n                prev_progress=prev_progress,\n                curr_progress=next_progress,\n                prev_stage=prev_stage,\n                curr_stage=next_stage,\n                best_progress=best_progress_state,\n                best_stage=best_stage_state,\n            )\n\n            shaping = shaping_coef * (discount * mono_next_progress - mono_prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=mono_next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            out[i] = (\n                stage_penalty\n                + shaping\n                + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n                - float(step_penalty)\n            )\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            lower_entry_xy_dist_state = lower_entry_xy_next\n            best_progress_state = best_progress_next\n            best_stage_state = best_stage_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n                lower_entry_xy_dist_state = None\n                best_progress_state = None\n                best_stage_state = None\n        return out\n\n    def compute_v23_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V23 dataset rewards: v8-style without release event.\"\"\"\n        return self.compute_v7_dataset_rewards(\n            ds=ds,\n            discount=discount,\n            terminal_bonus=terminal_bonus,\n            shaping_coef=shaping_coef,\n        )\n\n    def compute_v24_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V24 dataset rewards: v23 + offline next-touch target-cube tracking.\"\"\"\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v24\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        target_state: Optional[int] = None\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n            gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=gp)\n            prev_env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n            next_env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n\n            prev_target = target_state\n            if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                prev_target = self._first_incomplete_cube_idx(prev_env)\n            touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob_i)\n            if touched_prev is not None:\n                prev_target = touched_prev\n            if prev_target is not None:\n                prev_env.active_cube_override = int(prev_target)\n\n            curr_target = prev_target\n            if curr_target is None or next_env.cubes[int(curr_target)].is_at_goal():\n                curr_target = self._first_incomplete_cube_idx(next_env)\n            touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n            if touched_curr is not None:\n                curr_target = touched_curr\n            if curr_target is not None:\n                next_env.active_cube_override = int(curr_target)\n\n            prev_progress, _ = progress_v7(prev_env)\n            next_progress, next_stage = progress_v7(next_env)\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(stage=next_stage, num_cubes=self.num_cubes, num_substages=3)\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n            target_state = curr_target\n            if terminals_data is not None and bool(terminals_data[i]):\n                target_state = None\n        return out\n\n    def compute_v25_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V25 dataset rewards: same as v24.\"\"\"\n        return self.compute_v24_dataset_rewards(\n            ds=ds,\n            discount=discount,\n            terminal_bonus=terminal_bonus,\n            shaping_coef=shaping_coef,\n        )\n\n    def compute_v26_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V26 dataset rewards: identical to v25.\"\"\"\n        return self.compute_v25_dataset_rewards(\n            ds=ds,\n            discount=discount,\n            terminal_bonus=terminal_bonus,\n            shaping_coef=shaping_coef,\n        )\n\n    def compute_v27_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V27 dataset rewards: v26 with pose-aligned reach/grasp-secure progress.\"\"\"\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v27\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        target_state: Optional[int] = None\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n            gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_target = target_state\n            if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                prev_target = self._first_incomplete_cube_idx(prev_env)\n            touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob_i)\n            if touched_prev is not None:\n                prev_target = touched_prev\n            if prev_target is not None:\n                prev_env.active_cube_override = int(prev_target)\n\n            curr_target = prev_target\n            if curr_target is None or next_env.cubes[int(curr_target)].is_at_goal():\n                curr_target = self._first_incomplete_cube_idx(next_env)\n            touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n            if touched_curr is not None:\n                curr_target = touched_curr\n            if curr_target is not None:\n                next_env.active_cube_override = int(curr_target)\n\n            prev_progress, _ = progress_v27(prev_env)\n            next_progress, next_stage = progress_v27(next_env)\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(stage=next_stage, num_cubes=self.num_cubes, num_substages=3)\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n            target_state = curr_target\n            if terminals_data is not None and bool(terminals_data[i]):\n                target_state = None\n        return out\n\n    def compute_v28_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V28 dataset rewards: v27 + explicit post-completion clear stage.\"\"\"\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v28\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        target_state: Optional[int] = None\n        clear_pending_state = False\n        clear_cube_idx_state: Optional[int] = None\n        clear_cube_z_state: Optional[float] = None\n\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n            gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_target = target_state\n            if clear_pending_state and clear_cube_idx_state is not None:\n                prev_env.clear_cube_idx = int(clear_cube_idx_state)\n                prev_env.clear_cube_z = clear_cube_z_state\n            else:\n                if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                    prev_target = self._first_incomplete_cube_idx(prev_env)\n                touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob_i)\n                if touched_prev is not None:\n                    prev_target = touched_prev\n                if prev_target is not None:\n                    prev_env.active_cube_override = int(prev_target)\n\n            curr_target = prev_target\n            clear_pending_next = clear_pending_state\n            clear_cube_idx_next = clear_cube_idx_state\n            clear_cube_z_next = clear_cube_z_state\n\n            if clear_pending_state and clear_cube_idx_state is not None:\n                if _v28_clear_done(next_env, int(clear_cube_idx_state), clear_cube_z_state):\n                    clear_pending_next = False\n                    clear_cube_idx_next = None\n                    clear_cube_z_next = None\n                    curr_target = self._first_incomplete_cube_idx(next_env)\n                    touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n                    if touched_curr is not None:\n                        curr_target = touched_curr\n                if clear_pending_next and clear_cube_idx_next is not None:\n                    next_env.clear_cube_idx = int(clear_cube_idx_next)\n                    next_env.clear_cube_z = clear_cube_z_next\n                elif curr_target is not None:\n                    next_env.active_cube_override = int(curr_target)\n            else:\n                if curr_target is None or next_env.cubes[int(curr_target)].is_at_goal():\n                    curr_target = self._first_incomplete_cube_idx(next_env)\n                touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n                if touched_curr is not None:\n                    curr_target = touched_curr\n\n                completed_idxs = [\n                    idx for idx in range(self.num_cubes)\n                    if _v28_is_place_release_completion(prev_env, next_env, idx)\n                ]\n                if completed_idxs:\n                    completion_idx = completed_idxs[0]\n                    if curr_target is not None and int(curr_target) in completed_idxs:\n                        completion_idx = int(curr_target)\n                    clear_pending_next = True\n                    clear_cube_idx_next = int(completion_idx)\n                    clear_cube_z_next = float(next_env.cubes[int(completion_idx)].position[2])\n                    next_env.clear_cube_idx = int(completion_idx)\n                    next_env.clear_cube_z = clear_cube_z_next\n                elif curr_target is not None:\n                    next_env.active_cube_override = int(curr_target)\n\n            prev_progress, _ = progress_v28(prev_env)\n            next_progress, next_stage = progress_v28(next_env)\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(stage=next_stage, num_cubes=self.num_cubes, num_substages=4)\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            target_state = curr_target\n            clear_pending_state = clear_pending_next\n            clear_cube_idx_state = clear_cube_idx_next\n            clear_cube_z_state = clear_cube_z_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                target_state = None\n                clear_pending_state = False\n                clear_cube_idx_state = None\n                clear_cube_z_state = None\n        return out\n\n    def compute_v29_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V29 dataset rewards: enforce place -> release -> lift before switching cube.\"\"\"\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v29\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        target_state: Optional[int] = None\n        place_pending_state = False\n        place_cube_idx_state: Optional[int] = None\n        place_cube_z_state: Optional[float] = None\n        release_ee_z_state: Optional[float] = None\n\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n            gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_target = target_state\n            if place_pending_state and place_cube_idx_state is not None:\n                prev_target = int(place_cube_idx_state)\n                prev_env.place_cube_idx = int(place_cube_idx_state)\n                prev_env.place_cube_z = place_cube_z_state\n                prev_env.release_ee_z = release_ee_z_state\n                prev_env.active_cube_override = int(place_cube_idx_state)\n            else:\n                if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                    prev_target = self._first_incomplete_cube_idx(prev_env)\n                touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob_i)\n                if touched_prev is not None:\n                    prev_target = touched_prev\n                if prev_target is not None:\n                    prev_env.active_cube_override = int(prev_target)\n\n            curr_target = prev_target\n            place_pending_next = place_pending_state\n            place_cube_idx_next = place_cube_idx_state\n            place_cube_z_next = place_cube_z_state\n            release_ee_z_next = release_ee_z_state\n\n            if place_pending_state and place_cube_idx_state is not None:\n                cube_idx = int(place_cube_idx_state)\n                if not next_env.cubes[cube_idx].is_at_goal():\n                    # Rolled/left goal before clean release+lift: force re-place same cube.\n                    place_pending_next = False\n                    place_cube_idx_next = None\n                    place_cube_z_next = None\n                    release_ee_z_next = None\n                    curr_target = cube_idx\n                    next_env.active_cube_override = cube_idx\n                else:\n                    curr_target = cube_idx\n                    if release_ee_z_state is None:\n                        if _v29_release_condition(next_env, cube_idx):\n                            if next_env.gripper_pos is not None:\n                                release_ee_z_next = float(next_env.gripper_pos[2])\n                            else:\n                                release_ee_z_next = float(next_env.cubes[cube_idx].position[2])\n                    else:\n                        if _v29_lift_done(next_env, release_ee_z_state):\n                            place_pending_next = False\n                            place_cube_idx_next = None\n                            place_cube_z_next = None\n                            release_ee_z_next = None\n                            curr_target = self._first_incomplete_cube_idx(next_env)\n                            touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n                            if touched_curr is not None:\n                                curr_target = touched_curr\n                    if place_pending_next:\n                        next_env.place_cube_idx = cube_idx\n                        next_env.place_cube_z = place_cube_z_state\n                        next_env.release_ee_z = release_ee_z_next\n                        next_env.active_cube_override = cube_idx\n                    elif curr_target is not None:\n                        next_env.active_cube_override = int(curr_target)\n            else:\n                if curr_target is None or next_env.cubes[int(curr_target)].is_at_goal():\n                    curr_target = self._first_incomplete_cube_idx(next_env)\n                touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n                if touched_curr is not None:\n                    curr_target = touched_curr\n\n                if curr_target is not None:\n                    cube_idx = int(curr_target)\n                    entered_goal = (not prev_env.cubes[cube_idx].is_at_goal()) and next_env.cubes[cube_idx].is_at_goal()\n                    if entered_goal:\n                        place_pending_next = True\n                        place_cube_idx_next = cube_idx\n                        place_cube_z_next = float(next_env.cubes[cube_idx].position[2])\n                        release_ee_z_next = None\n                        next_env.place_cube_idx = cube_idx\n                        next_env.place_cube_z = place_cube_z_next\n                        next_env.release_ee_z = None\n                        next_env.active_cube_override = cube_idx\n                    else:\n                        next_env.active_cube_override = cube_idx\n\n            prev_progress, _ = progress_v29(prev_env)\n            next_progress, next_stage = progress_v29(next_env)\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(stage=next_stage, num_cubes=self.num_cubes, num_substages=5)\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            target_state = curr_target\n            place_pending_state = place_pending_next\n            place_cube_idx_state = place_cube_idx_next\n            place_cube_z_state = place_cube_z_next\n            release_ee_z_state = release_ee_z_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                target_state = None\n                place_pending_state = False\n                place_cube_idx_state = None\n                place_cube_z_state = None\n                release_ee_z_state = None\n        return out\n\n    def compute_v30_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"V30 dataset rewards: v29 state machine with stricter stage-1 progress.\"\"\"\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v30\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        target_state: Optional[int] = None\n        place_pending_state = False\n        place_cube_idx_state: Optional[int] = None\n        place_cube_z_state: Optional[float] = None\n        release_ee_z_state: Optional[float] = None\n\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n            gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_target = target_state\n            if place_pending_state and place_cube_idx_state is not None:\n                prev_target = int(place_cube_idx_state)\n                prev_env.place_cube_idx = int(place_cube_idx_state)\n                prev_env.place_cube_z = place_cube_z_state\n                prev_env.release_ee_z = release_ee_z_state\n                prev_env.active_cube_override = int(place_cube_idx_state)\n            else:\n                if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                    prev_target = self._first_incomplete_cube_idx(prev_env)\n                touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob_i)\n                if touched_prev is not None:\n                    prev_target = touched_prev\n                if prev_target is not None:\n                    prev_env.active_cube_override = int(prev_target)\n\n            curr_target = prev_target\n            place_pending_next = place_pending_state\n            place_cube_idx_next = place_cube_idx_state\n            place_cube_z_next = place_cube_z_state\n            release_ee_z_next = release_ee_z_state\n\n            if place_pending_state and place_cube_idx_state is not None:\n                cube_idx = int(place_cube_idx_state)\n                if not next_env.cubes[cube_idx].is_at_goal():\n                    place_pending_next = False\n                    place_cube_idx_next = None\n                    place_cube_z_next = None\n                    release_ee_z_next = None\n                    curr_target = cube_idx\n                    next_env.active_cube_override = cube_idx\n                else:\n                    curr_target = cube_idx\n                    if release_ee_z_state is None:\n                        if _v29_release_condition(next_env, cube_idx):\n                            if next_env.gripper_pos is not None:\n                                release_ee_z_next = float(next_env.gripper_pos[2])\n                            else:\n                                release_ee_z_next = float(next_env.cubes[cube_idx].position[2])\n                    else:\n                        if _v29_lift_done(next_env, release_ee_z_state):\n                            place_pending_next = False\n                            place_cube_idx_next = None\n                            place_cube_z_next = None\n                            release_ee_z_next = None\n                            curr_target = self._first_incomplete_cube_idx(next_env)\n                            touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n                            if touched_curr is not None:\n                                curr_target = touched_curr\n                    if place_pending_next:\n                        next_env.place_cube_idx = cube_idx\n                        next_env.place_cube_z = place_cube_z_state\n                        next_env.release_ee_z = release_ee_z_next\n                        next_env.active_cube_override = cube_idx\n                    elif curr_target is not None:\n                        next_env.active_cube_override = int(curr_target)\n            else:\n                if curr_target is None or next_env.cubes[int(curr_target)].is_at_goal():\n                    curr_target = self._first_incomplete_cube_idx(next_env)\n                touched_curr = self._detect_touched_cube_idx(next_env, next_ob_i)\n                if touched_curr is not None:\n                    curr_target = touched_curr\n\n                if curr_target is not None:\n                    cube_idx = int(curr_target)\n                    entered_goal = (not prev_env.cubes[cube_idx].is_at_goal()) and next_env.cubes[cube_idx].is_at_goal()\n                    if entered_goal:\n                        place_pending_next = True\n                        place_cube_idx_next = cube_idx\n                        place_cube_z_next = float(next_env.cubes[cube_idx].position[2])\n                        release_ee_z_next = None\n                        next_env.place_cube_idx = cube_idx\n                        next_env.place_cube_z = place_cube_z_next\n                        next_env.release_ee_z = None\n                        next_env.active_cube_override = cube_idx\n                    else:\n                        next_env.active_cube_override = cube_idx\n\n            prev_progress, _ = progress_v30(prev_env)\n            next_progress, next_stage = progress_v30(next_env)\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(stage=next_stage, num_cubes=self.num_cubes, num_substages=5)\n            out[i] = stage_penalty + shaping + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            target_state = curr_target\n            place_pending_state = place_pending_next\n            place_cube_idx_state = place_cube_idx_next\n            place_cube_z_state = place_cube_z_next\n            release_ee_z_state = release_ee_z_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                target_state = None\n                place_pending_state = False\n                place_cube_idx_state = None\n                place_cube_z_state = None\n                release_ee_z_state = None\n        return out\n\n    def compute_v14_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        self._require_keys(ds, ['qpos', 'next_qpos', 'rewards'], \"v14\")\n        qpos_data = ds['qpos']\n        qvel_data = ds.get('qvel', None)\n        next_qpos_data = ds['next_qpos']\n        next_qvel_data = ds.get('next_qvel', None)\n        obs_data = ds.get('observations', None)\n        next_obs_data = ds.get('next_observations', None)\n        terminals_data = ds.get('terminals', ds.get('dones', ds.get('dones_float', None)))\n\n        prev_subtask_state: Optional[int] = None\n        lower_entry_z_state: Optional[float] = None\n        lower_entry_cube_idx_state: Optional[int] = None\n\n        out = np.zeros(len(qpos_data), dtype=np.float32)\n        for i in range(len(qpos_data)):\n            qvel = qvel_data[i] if qvel_data is not None else None\n            next_qvel = next_qvel_data[i] if next_qvel_data is not None else None\n            prev_ob_i = obs_data[i] if obs_data is not None else None\n            next_ob_i = next_obs_data[i] if next_obs_data is not None else None\n\n            prev_gp = extract_gripper_pos(prev_ob_i) if prev_ob_i is not None else None\n            next_gp = extract_gripper_pos(next_ob_i) if next_ob_i is not None else None\n\n            prev_env = self._make_env()\n            prev_env.load_state(qpos_data[i], qvel=qvel, gripper_pos=prev_gp)\n            next_env = self._make_env()\n            next_env.load_state(next_qpos_data[i], qvel=next_qvel, gripper_pos=next_gp)\n\n            prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev = _progress_v11(\n                prev_env,\n                prev_subtask=prev_subtask_state,\n                prev_env=None,\n                lower_entry_z=lower_entry_z_state,\n                lower_entry_cube_idx=lower_entry_cube_idx_state,\n            )\n            next_progress, next_stage, next_subtask, lower_entry_z_next, lower_entry_cube_idx_next = _progress_v11(\n                next_env,\n                prev_subtask=prev_subtask_resolved,\n                prev_env=prev_env,\n                lower_entry_z=lower_entry_z_prev,\n                lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            )\n\n            shaping = shaping_coef * (discount * next_progress - prev_progress)\n            stage_penalty = self._stage_penalty_from_stage(\n                stage=next_stage,\n                num_cubes=self.num_cubes,\n                num_substages=4,\n            )\n            transition_event = self._v13_subtask_transition_event(\n                prev_subtask=prev_subtask_resolved,\n                next_subtask=next_subtask,\n            )\n            out[i] = stage_penalty + shaping + transition_event + self._success_bonus_post(next_qpos_data[i], next_qvel, terminal_bonus)\n\n            prev_subtask_state = next_subtask\n            lower_entry_z_state = lower_entry_z_next\n            lower_entry_cube_idx_state = lower_entry_cube_idx_next\n            if terminals_data is not None and bool(terminals_data[i]):\n                prev_subtask_state = None\n                lower_entry_z_state = None\n                lower_entry_cube_idx_state = None\n        return out\n\n    def compute_dataset_rewards(\n        self,\n        ds: Dict[str, np.ndarray],\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> np.ndarray:\n        \"\"\"Compute dense rewards for a transition dataset via version-specific handlers.\"\"\"\n        saved_episode_init_positions = self.episode_init_positions\n        self.episode_init_positions = None\n        try:\n            if self.version == 'v1':\n                return self.compute_v1_dataset_rewards(ds, discount=discount, terminal_bonus=terminal_bonus)\n            if self.version == 'v2':\n                return self.compute_v2_dataset_rewards(ds, discount=discount, terminal_bonus=terminal_bonus)\n            if self.version == 'v3':\n                return self.compute_v3_dataset_rewards(ds, discount=discount, terminal_bonus=terminal_bonus)\n            if self.version == 'v4':\n                return self.compute_v4_dataset_rewards(ds, discount=discount, terminal_bonus=terminal_bonus, shaping_coef=shaping_coef)\n            if self.version == 'v5':\n                return self.compute_v5_dataset_rewards(ds, discount=discount, terminal_bonus=terminal_bonus, shaping_coef=shaping_coef)\n            if self.version == 'v6':\n                return self.compute_v6_dataset_rewards(ds, discount=discount, terminal_bonus=terminal_bonus, shaping_coef=shaping_coef)\n            if self.version == 'v7':\n                return self.compute_v7_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v8':\n                return self.compute_v8_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v9':\n                return self.compute_v9_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v10':\n                return self.compute_v10_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v11':\n                return self.compute_v11_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v12':\n                return self.compute_v12_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v13':\n                return self.compute_v13_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v14':\n                return self.compute_v14_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v15':\n                return self.compute_v15_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v16':\n                return self.compute_v16_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v17':\n                return self.compute_v17_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v18':\n                return self.compute_v18_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version in ('v20', 'v21'):\n                return self.compute_v20_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v22':\n                return self.compute_v22_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v23':\n                return self.compute_v23_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v24':\n                return self.compute_v24_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v25':\n                return self.compute_v25_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v26':\n                return self.compute_v26_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v27':\n                return self.compute_v27_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v28':\n                return self.compute_v28_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v29':\n                return self.compute_v29_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            if self.version == 'v30':\n                return self.compute_v30_dataset_rewards(\n                    ds,\n                    discount=discount,\n                    terminal_bonus=terminal_bonus,\n                    shaping_coef=shaping_coef,\n                )\n            raise ValueError(f\"Unknown version for dataset rewards: {self.version}\")\n        finally:\n            self.episode_init_positions = saved_episode_init_positions\n\n    def compute_v1_online_reward(\n        self,\n        curr_qpos: np.ndarray,\n        env_reward: float = 0.0,\n        curr_ob: Optional[np.ndarray] = None,\n        terminal_bonus: float = 50.0,\n        **_,\n    ) -> float:\n        return float(self.compute_potential(curr_qpos) + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v2_online_reward(\n        self,\n        curr_qpos: np.ndarray,\n        env_reward: float = 0.0,\n        curr_ob: Optional[np.ndarray] = None,\n        terminal_bonus: float = 50.0,\n        **_,\n    ) -> float:\n        return float(self.compute_potential(curr_qpos) + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v3_online_reward(\n        self,\n        curr_qpos: np.ndarray,\n        env_reward: float = 0.0,\n        curr_ob: Optional[np.ndarray] = None,\n        terminal_bonus: float = 50.0,\n        **_,\n    ) -> float:\n        gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n        return float(self.compute_potential(curr_qpos, gripper_pos=gp) + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v4_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_progress, _ = self.compute_progress(prev_qpos)\n        curr_progress, _ = self.compute_progress(curr_qpos)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        return float(env_reward + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v5_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n        prev_progress, _ = self.compute_progress(prev_qpos, gripper_pos=prev_gp)\n        curr_progress, _ = self.compute_progress(curr_qpos, gripper_pos=curr_gp)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        return float(env_reward + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v6_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n        prev_progress, _ = self.compute_progress(prev_qpos, gripper_pos=prev_gp)\n        curr_progress, _ = self.compute_progress(curr_qpos, gripper_pos=curr_gp)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        return float(env_reward + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v7_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n        prev_progress, _ = self.compute_progress(prev_qpos, gripper_pos=prev_gp)\n        curr_progress, _ = self.compute_progress(curr_qpos, gripper_pos=curr_gp)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._v7_v8_stage_penalty(qpos=curr_qpos, ob=curr_ob)\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v8_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n        prev_progress, _ = self.compute_progress(prev_qpos, gripper_pos=prev_gp)\n        curr_progress, _ = self.compute_progress(curr_qpos, gripper_pos=curr_gp)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._v7_v8_stage_penalty(qpos=curr_qpos, ob=curr_ob)\n        event = self._v8_release_event(prev_qpos=prev_qpos, curr_qpos=curr_qpos, prev_ob=prev_ob, curr_ob=curr_ob)\n        return float(stage_penalty + shaping + event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v9_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        stage_penalty = self._v7_v8_stage_penalty(qpos=curr_qpos, ob=curr_ob)\n        return float(stage_penalty + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v10_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n        prev_progress, _ = self.compute_progress(prev_qpos, gripper_pos=prev_gp)\n        curr_progress, _ = self.compute_progress(curr_qpos, gripper_pos=curr_gp)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._v7_v8_stage_penalty(qpos=curr_qpos, ob=curr_ob)\n        event = self._v8_release_event(prev_qpos=prev_qpos, curr_qpos=curr_qpos, prev_ob=prev_ob, curr_ob=curr_ob)\n        return float(stage_penalty + shaping + event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v11_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev = _progress_v11(\n            prev_env,\n            prev_subtask=self._v11_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v11_lower_entry_z,\n            lower_entry_cube_idx=self._v11_lower_entry_cube_idx,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr = _progress_v11(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        event = self._v8_release_event(prev_qpos=prev_qpos, curr_qpos=curr_qpos, prev_ob=prev_ob, curr_ob=curr_ob)\n\n        self._v11_prev_subtask = curr_subtask\n        self._v11_lower_entry_z = lower_entry_z_curr\n        self._v11_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        return float(stage_penalty + shaping + event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v12_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v12(\n            prev_env,\n            prev_subtask=self._v12_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v12_lower_entry_z,\n            lower_entry_cube_idx=self._v12_lower_entry_cube_idx,\n            lower_entry_xy_dist=self._v12_lower_entry_xy_dist,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr, lower_entry_xy_curr = _progress_v12(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            lower_entry_xy_dist=lower_entry_xy_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        event = self._v8_release_event(prev_qpos=prev_qpos, curr_qpos=curr_qpos, prev_ob=prev_ob, curr_ob=curr_ob)\n\n        self._v12_prev_subtask = curr_subtask\n        self._v12_lower_entry_z = lower_entry_z_curr\n        self._v12_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        self._v12_lower_entry_xy_dist = lower_entry_xy_curr\n        return float(stage_penalty + shaping + event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v13_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev = _progress_v11(\n            prev_env,\n            prev_subtask=self._v13_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v13_lower_entry_z,\n            lower_entry_cube_idx=self._v13_lower_entry_cube_idx,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr = _progress_v11(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        # transition_event = self._v13_subtask_transition_event(\n        #     prev_subtask=prev_subtask_resolved,\n        #     next_subtask=curr_subtask,\n        # )\n\n        self._v13_prev_subtask = curr_subtask\n        self._v13_lower_entry_z = lower_entry_z_curr\n        self._v13_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v14_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev = _progress_v11(\n            prev_env,\n            prev_subtask=self._v14_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v14_lower_entry_z,\n            lower_entry_cube_idx=self._v14_lower_entry_cube_idx,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr = _progress_v11(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        transition_event = self._v13_subtask_transition_event(\n            prev_subtask=prev_subtask_resolved,\n            next_subtask=curr_subtask,\n        )\n\n        self._v14_prev_subtask = curr_subtask\n        self._v14_lower_entry_z = lower_entry_z_curr\n        self._v14_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        return float(stage_penalty + shaping + transition_event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v15_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v15(\n            prev_env,\n            prev_subtask=self._v15_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v15_lower_entry_z,\n            lower_entry_cube_idx=self._v15_lower_entry_cube_idx,\n            lower_entry_xy_dist=self._v15_lower_entry_xy_dist,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr, lower_entry_xy_curr = _progress_v15(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            lower_entry_xy_dist=lower_entry_xy_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        event = self._v8_release_event(prev_qpos=prev_qpos, curr_qpos=curr_qpos, prev_ob=prev_ob, curr_ob=curr_ob)\n\n        self._v15_prev_subtask = curr_subtask\n        self._v15_lower_entry_z = lower_entry_z_curr\n        self._v15_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        self._v15_lower_entry_xy_dist = lower_entry_xy_curr\n        return float(stage_penalty + shaping + event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v16_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v15(\n            prev_env,\n            prev_subtask=self._v16_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v16_lower_entry_z,\n            lower_entry_cube_idx=self._v16_lower_entry_cube_idx,\n            lower_entry_xy_dist=self._v16_lower_entry_xy_dist,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr, lower_entry_xy_curr = _progress_v15(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            lower_entry_xy_dist=lower_entry_xy_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n\n        self._v16_prev_subtask = curr_subtask\n        self._v16_lower_entry_z = lower_entry_z_curr\n        self._v16_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        self._v16_lower_entry_xy_dist = lower_entry_xy_curr\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v17_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v15(\n            prev_env,\n            prev_subtask=self._v17_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v17_lower_entry_z,\n            lower_entry_cube_idx=self._v17_lower_entry_cube_idx,\n            lower_entry_xy_dist=self._v17_lower_entry_xy_dist,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr, lower_entry_xy_curr = _progress_v15(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            lower_entry_xy_dist=lower_entry_xy_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        transition_event = self._v13_subtask_transition_event(\n            prev_subtask=prev_subtask_resolved,\n            next_subtask=curr_subtask,\n        )\n\n        self._v17_prev_subtask = curr_subtask\n        self._v17_lower_entry_z = lower_entry_z_curr\n        self._v17_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        self._v17_lower_entry_xy_dist = lower_entry_xy_curr\n        return float(stage_penalty + shaping + transition_event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v18_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v18(\n            prev_env,\n            prev_subtask=self._v18_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v18_lower_entry_z,\n            lower_entry_cube_idx=self._v18_lower_entry_cube_idx,\n            lower_entry_xy_dist=self._v18_lower_entry_xy_dist,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr, lower_entry_xy_curr = _progress_v18(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            lower_entry_xy_dist=lower_entry_xy_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        event = self._v8_release_event(prev_qpos=prev_qpos, curr_qpos=curr_qpos, prev_ob=prev_ob, curr_ob=curr_ob)\n\n        self._v18_prev_subtask = curr_subtask\n        self._v18_lower_entry_z = lower_entry_z_curr\n        self._v18_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        self._v18_lower_entry_xy_dist = lower_entry_xy_curr\n        return float(stage_penalty + shaping + event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v20_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        if prev_gripper_gap_m is not None:\n            self._v20_gap_open_ref_m = max(self._v20_gap_open_ref_m, float(prev_gripper_gap_m), V20_GRIPPER_GAP_OPEN_FLOOR)\n        elif curr_gripper_gap_m is not None:\n            self._v20_gap_open_ref_m = max(self._v20_gap_open_ref_m, float(curr_gripper_gap_m), V20_GRIPPER_GAP_OPEN_FLOOR)\n\n        prev_env = self._make_env()\n        prev_env.load_state(\n            prev_qpos,\n            qvel=None,\n            gripper_pos=prev_gp,\n            gripper_gap_m=prev_gripper_gap_m,\n            gripper_gap_open_ref_m=self._v20_gap_open_ref_m,\n            left_gripper_pos=prev_left_gripper_pos,\n            right_gripper_pos=prev_right_gripper_pos,\n        )\n        prev_env.use_pad_reach = self.version in (\"v21\", \"v22\", \"v23\")\n        prev_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n        curr_env = self._make_env()\n        curr_env.load_state(\n            curr_qpos,\n            qvel=None,\n            gripper_pos=curr_gp,\n            gripper_gap_m=curr_gripper_gap_m,\n            gripper_gap_open_ref_m=self._v20_gap_open_ref_m,\n            left_gripper_pos=curr_left_gripper_pos,\n            right_gripper_pos=curr_right_gripper_pos,\n        )\n        curr_env.use_pad_reach = self.version in (\"v21\", \"v22\", \"v23\")\n        curr_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n\n        prev_progress, _, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v18(\n            prev_env,\n            prev_subtask=self._v20_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v20_lower_entry_z,\n            lower_entry_cube_idx=self._v20_lower_entry_cube_idx,\n            lower_entry_xy_dist=self._v20_lower_entry_xy_dist,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr, lower_entry_xy_curr = _progress_v18(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            lower_entry_xy_dist=lower_entry_xy_prev,\n        )\n\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        event = self._v8_release_event(prev_qpos=prev_qpos, curr_qpos=curr_qpos, prev_ob=prev_ob, curr_ob=curr_ob)\n\n        self._v20_prev_subtask = curr_subtask\n        self._v20_lower_entry_z = lower_entry_z_curr\n        self._v20_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        self._v20_lower_entry_xy_dist = lower_entry_xy_curr\n        return float(stage_penalty + shaping + event + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v22_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        step_penalty: float = 0.0,\n        **_,\n    ) -> float:\n        \"\"\"V22 online rewards: v21 semantics with monotonic anti-cycling.\"\"\"\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        if prev_gripper_gap_m is not None:\n            self._v22_gap_open_ref_m = max(self._v22_gap_open_ref_m, float(prev_gripper_gap_m), V20_GRIPPER_GAP_OPEN_FLOOR)\n        elif curr_gripper_gap_m is not None:\n            self._v22_gap_open_ref_m = max(self._v22_gap_open_ref_m, float(curr_gripper_gap_m), V20_GRIPPER_GAP_OPEN_FLOOR)\n\n        prev_env = self._make_env()\n        prev_env.load_state(\n            prev_qpos,\n            qvel=None,\n            gripper_pos=prev_gp,\n            gripper_gap_m=prev_gripper_gap_m,\n            gripper_gap_open_ref_m=self._v22_gap_open_ref_m,\n            left_gripper_pos=prev_left_gripper_pos,\n            right_gripper_pos=prev_right_gripper_pos,\n        )\n        prev_env.use_pad_reach = True\n        prev_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n        curr_env = self._make_env()\n        curr_env.load_state(\n            curr_qpos,\n            qvel=None,\n            gripper_pos=curr_gp,\n            gripper_gap_m=curr_gripper_gap_m,\n            gripper_gap_open_ref_m=self._v22_gap_open_ref_m,\n            left_gripper_pos=curr_left_gripper_pos,\n            right_gripper_pos=curr_right_gripper_pos,\n        )\n        curr_env.use_pad_reach = True\n        curr_env.pad_reach_threshold = V21_REACH_PAD_CUBE_DIST_THRESHOLD\n\n        prev_progress, prev_stage, prev_subtask_resolved, lower_entry_z_prev, lower_entry_cube_idx_prev, lower_entry_xy_prev = _progress_v18(\n            prev_env,\n            prev_subtask=self._v22_prev_subtask,\n            prev_env=None,\n            lower_entry_z=self._v22_lower_entry_z,\n            lower_entry_cube_idx=self._v22_lower_entry_cube_idx,\n            lower_entry_xy_dist=self._v22_lower_entry_xy_dist,\n        )\n        curr_progress, curr_stage, curr_subtask, lower_entry_z_curr, lower_entry_cube_idx_curr, lower_entry_xy_curr = _progress_v18(\n            curr_env,\n            prev_subtask=prev_subtask_resolved,\n            prev_env=prev_env,\n            lower_entry_z=lower_entry_z_prev,\n            lower_entry_cube_idx=lower_entry_cube_idx_prev,\n            lower_entry_xy_dist=lower_entry_xy_prev,\n        )\n        mono_prev_progress, mono_curr_progress, mono_curr_stage, best_progress_next, best_stage_next = self._monotonic_progress_stage(\n            prev_progress=prev_progress,\n            curr_progress=curr_progress,\n            prev_stage=prev_stage,\n            curr_stage=curr_stage,\n            best_progress=self._v22_best_progress,\n            best_stage=self._v22_best_stage,\n        )\n\n        shaping = shaping_coef * (discount * mono_curr_progress - mono_prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=mono_curr_stage, num_cubes=self.num_cubes, num_substages=4)\n        self._v22_prev_subtask = curr_subtask\n        self._v22_lower_entry_z = lower_entry_z_curr\n        self._v22_lower_entry_cube_idx = lower_entry_cube_idx_curr\n        self._v22_lower_entry_xy_dist = lower_entry_xy_curr\n        self._v22_best_progress = best_progress_next\n        self._v22_best_stage = best_stage_next\n        return float(\n            stage_penalty\n            + shaping\n            + self._success_bonus_post(curr_qpos, None, terminal_bonus)\n            - float(step_penalty)\n        )\n\n    def compute_v23_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        return self.compute_v7_online_reward(\n            prev_qpos=prev_qpos,\n            curr_qpos=curr_qpos,\n            env_reward=env_reward,\n            prev_ob=prev_ob,\n            curr_ob=curr_ob,\n            discount=discount,\n            terminal_bonus=terminal_bonus,\n            shaping_coef=shaping_coef,\n        )\n\n    def compute_v24_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        prev_env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n        curr_env.grasp_lift_threshold_override = V6_GRASP_LIFT_TARGET\n\n        prev_target = self._v24_target_cube_idx\n        if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n            prev_target = self._first_incomplete_cube_idx(prev_env)\n        touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob)\n        if touched_prev is not None:\n            prev_target = touched_prev\n        if prev_target is not None:\n            prev_env.active_cube_override = int(prev_target)\n\n        curr_target = prev_target\n        if curr_target is None or curr_env.cubes[int(curr_target)].is_at_goal():\n            curr_target = self._first_incomplete_cube_idx(curr_env)\n        touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n        if touched_curr is not None:\n            curr_target = touched_curr\n        if curr_target is not None:\n            curr_env.active_cube_override = int(curr_target)\n\n        prev_progress, _ = progress_v7(prev_env)\n        curr_progress, curr_stage = progress_v7(curr_env)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=3)\n        self._v24_target_cube_idx = curr_target\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v25_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        \"\"\"V25 online reward core is v24; shaping delivery differs in trainer via chunking.\"\"\"\n        return self.compute_v24_online_reward(\n            prev_qpos=prev_qpos,\n            curr_qpos=curr_qpos,\n            env_reward=env_reward,\n            prev_ob=prev_ob,\n            curr_ob=curr_ob,\n            prev_gripper_gap_m=prev_gripper_gap_m,\n            curr_gripper_gap_m=curr_gripper_gap_m,\n            prev_left_gripper_pos=prev_left_gripper_pos,\n            prev_right_gripper_pos=prev_right_gripper_pos,\n            curr_left_gripper_pos=curr_left_gripper_pos,\n            curr_right_gripper_pos=curr_right_gripper_pos,\n            discount=discount,\n            terminal_bonus=terminal_bonus,\n            shaping_coef=shaping_coef,\n        )\n\n    def compute_v26_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        \"\"\"V26 online reward: identical to v25.\"\"\"\n        return self.compute_v25_online_reward(\n            prev_qpos=prev_qpos,\n            curr_qpos=curr_qpos,\n            env_reward=env_reward,\n            prev_ob=prev_ob,\n            curr_ob=curr_ob,\n            prev_gripper_gap_m=prev_gripper_gap_m,\n            curr_gripper_gap_m=curr_gripper_gap_m,\n            prev_left_gripper_pos=prev_left_gripper_pos,\n            prev_right_gripper_pos=prev_right_gripper_pos,\n            curr_left_gripper_pos=curr_left_gripper_pos,\n            curr_right_gripper_pos=curr_right_gripper_pos,\n            discount=discount,\n            terminal_bonus=terminal_bonus,\n            shaping_coef=shaping_coef,\n        )\n\n    def compute_v27_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_target = self._v24_target_cube_idx\n        if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n            prev_target = self._first_incomplete_cube_idx(prev_env)\n        touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob)\n        if touched_prev is not None:\n            prev_target = touched_prev\n        if prev_target is not None:\n            prev_env.active_cube_override = int(prev_target)\n\n        curr_target = prev_target\n        if curr_target is None or curr_env.cubes[int(curr_target)].is_at_goal():\n            curr_target = self._first_incomplete_cube_idx(curr_env)\n        touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n        if touched_curr is not None:\n            curr_target = touched_curr\n        if curr_target is not None:\n            curr_env.active_cube_override = int(curr_target)\n\n        prev_progress, _ = progress_v27(prev_env)\n        curr_progress, curr_stage = progress_v27(curr_env)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=3)\n        self._v24_target_cube_idx = curr_target\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v28_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_target = self._v24_target_cube_idx\n        if self._v28_clear_pending and self._v28_clear_cube_idx is not None:\n            prev_env.clear_cube_idx = int(self._v28_clear_cube_idx)\n            prev_env.clear_cube_z = self._v28_clear_cube_z\n        else:\n            if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                prev_target = self._first_incomplete_cube_idx(prev_env)\n            touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob)\n            if touched_prev is not None:\n                prev_target = touched_prev\n            if prev_target is not None:\n                prev_env.active_cube_override = int(prev_target)\n\n        curr_target = prev_target\n        clear_pending_next = self._v28_clear_pending\n        clear_cube_idx_next = self._v28_clear_cube_idx\n        clear_cube_z_next = self._v28_clear_cube_z\n\n        if self._v28_clear_pending and self._v28_clear_cube_idx is not None:\n            if _v28_clear_done(curr_env, int(self._v28_clear_cube_idx), self._v28_clear_cube_z):\n                clear_pending_next = False\n                clear_cube_idx_next = None\n                clear_cube_z_next = None\n                curr_target = self._first_incomplete_cube_idx(curr_env)\n                touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n                if touched_curr is not None:\n                    curr_target = touched_curr\n            if clear_pending_next and clear_cube_idx_next is not None:\n                curr_env.clear_cube_idx = int(clear_cube_idx_next)\n                curr_env.clear_cube_z = clear_cube_z_next\n            elif curr_target is not None:\n                curr_env.active_cube_override = int(curr_target)\n        else:\n            if curr_target is None or curr_env.cubes[int(curr_target)].is_at_goal():\n                curr_target = self._first_incomplete_cube_idx(curr_env)\n            touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n            if touched_curr is not None:\n                curr_target = touched_curr\n\n            completed_idxs = [\n                idx for idx in range(self.num_cubes)\n                if _v28_is_place_release_completion(prev_env, curr_env, idx)\n            ]\n            if completed_idxs:\n                completion_idx = completed_idxs[0]\n                if curr_target is not None and int(curr_target) in completed_idxs:\n                    completion_idx = int(curr_target)\n                clear_pending_next = True\n                clear_cube_idx_next = int(completion_idx)\n                clear_cube_z_next = float(curr_env.cubes[int(completion_idx)].position[2])\n                curr_env.clear_cube_idx = int(completion_idx)\n                curr_env.clear_cube_z = clear_cube_z_next\n            elif curr_target is not None:\n                curr_env.active_cube_override = int(curr_target)\n\n        prev_progress, _ = progress_v28(prev_env)\n        curr_progress, curr_stage = progress_v28(curr_env)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=4)\n\n        self._v24_target_cube_idx = curr_target\n        self._v28_clear_pending = bool(clear_pending_next)\n        self._v28_clear_cube_idx = None if clear_cube_idx_next is None else int(clear_cube_idx_next)\n        self._v28_clear_cube_z = None if clear_cube_z_next is None else float(clear_cube_z_next)\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v29_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_target = self._v24_target_cube_idx\n        if self._v29_place_pending and self._v29_place_cube_idx is not None:\n            prev_target = int(self._v29_place_cube_idx)\n            prev_env.place_cube_idx = int(self._v29_place_cube_idx)\n            prev_env.place_cube_z = self._v29_place_cube_z\n            prev_env.release_ee_z = self._v29_release_ee_z\n            prev_env.active_cube_override = int(self._v29_place_cube_idx)\n        else:\n            if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                prev_target = self._first_incomplete_cube_idx(prev_env)\n            touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob)\n            if touched_prev is not None:\n                prev_target = touched_prev\n            if prev_target is not None:\n                prev_env.active_cube_override = int(prev_target)\n\n        curr_target = prev_target\n        place_pending_next = self._v29_place_pending\n        place_cube_idx_next = self._v29_place_cube_idx\n        place_cube_z_next = self._v29_place_cube_z\n        release_ee_z_next = self._v29_release_ee_z\n\n        if self._v29_place_pending and self._v29_place_cube_idx is not None:\n            cube_idx = int(self._v29_place_cube_idx)\n            if not curr_env.cubes[cube_idx].is_at_goal():\n                place_pending_next = False\n                place_cube_idx_next = None\n                place_cube_z_next = None\n                release_ee_z_next = None\n                curr_target = cube_idx\n                curr_env.active_cube_override = cube_idx\n            else:\n                curr_target = cube_idx\n                if self._v29_release_ee_z is None:\n                    if _v29_release_condition(curr_env, cube_idx):\n                        if curr_env.gripper_pos is not None:\n                            release_ee_z_next = float(curr_env.gripper_pos[2])\n                        else:\n                            release_ee_z_next = float(curr_env.cubes[cube_idx].position[2])\n                else:\n                    if _v29_lift_done(curr_env, self._v29_release_ee_z):\n                        place_pending_next = False\n                        place_cube_idx_next = None\n                        place_cube_z_next = None\n                        release_ee_z_next = None\n                        curr_target = self._first_incomplete_cube_idx(curr_env)\n                        touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n                        if touched_curr is not None:\n                            curr_target = touched_curr\n                if place_pending_next:\n                    curr_env.place_cube_idx = cube_idx\n                    curr_env.place_cube_z = self._v29_place_cube_z\n                    curr_env.release_ee_z = release_ee_z_next\n                    curr_env.active_cube_override = cube_idx\n                elif curr_target is not None:\n                    curr_env.active_cube_override = int(curr_target)\n        else:\n            if curr_target is None or curr_env.cubes[int(curr_target)].is_at_goal():\n                curr_target = self._first_incomplete_cube_idx(curr_env)\n            touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n            if touched_curr is not None:\n                curr_target = touched_curr\n\n            if curr_target is not None:\n                cube_idx = int(curr_target)\n                entered_goal = (not prev_env.cubes[cube_idx].is_at_goal()) and curr_env.cubes[cube_idx].is_at_goal()\n                if entered_goal:\n                    place_pending_next = True\n                    place_cube_idx_next = cube_idx\n                    place_cube_z_next = float(curr_env.cubes[cube_idx].position[2])\n                    release_ee_z_next = None\n                    curr_env.place_cube_idx = cube_idx\n                    curr_env.place_cube_z = place_cube_z_next\n                    curr_env.release_ee_z = None\n                    curr_env.active_cube_override = cube_idx\n                else:\n                    curr_env.active_cube_override = cube_idx\n\n        prev_progress, _ = progress_v29(prev_env)\n        curr_progress, curr_stage = progress_v29(curr_env)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=5)\n\n        self._v24_target_cube_idx = curr_target\n        self._v29_place_pending = bool(place_pending_next)\n        self._v29_place_cube_idx = None if place_cube_idx_next is None else int(place_cube_idx_next)\n        self._v29_place_cube_z = None if place_cube_z_next is None else float(place_cube_z_next)\n        self._v29_release_ee_z = None if release_ee_z_next is None else float(release_ee_z_next)\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_v30_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n        **_,\n    ) -> float:\n        prev_gp = extract_gripper_pos(prev_ob) if prev_ob is not None else None\n        curr_gp = extract_gripper_pos(curr_ob) if curr_ob is not None else None\n\n        prev_env = self._make_env()\n        prev_env.load_state(prev_qpos, qvel=None, gripper_pos=prev_gp)\n        curr_env = self._make_env()\n        curr_env.load_state(curr_qpos, qvel=None, gripper_pos=curr_gp)\n\n        prev_target = self._v24_target_cube_idx\n        if self._v29_place_pending and self._v29_place_cube_idx is not None:\n            prev_target = int(self._v29_place_cube_idx)\n            prev_env.place_cube_idx = int(self._v29_place_cube_idx)\n            prev_env.place_cube_z = self._v29_place_cube_z\n            prev_env.release_ee_z = self._v29_release_ee_z\n            prev_env.active_cube_override = int(self._v29_place_cube_idx)\n        else:\n            if prev_target is None or prev_env.cubes[int(prev_target)].is_at_goal():\n                prev_target = self._first_incomplete_cube_idx(prev_env)\n            touched_prev = self._detect_touched_cube_idx(prev_env, prev_ob)\n            if touched_prev is not None:\n                prev_target = touched_prev\n            if prev_target is not None:\n                prev_env.active_cube_override = int(prev_target)\n\n        curr_target = prev_target\n        place_pending_next = self._v29_place_pending\n        place_cube_idx_next = self._v29_place_cube_idx\n        place_cube_z_next = self._v29_place_cube_z\n        release_ee_z_next = self._v29_release_ee_z\n\n        if self._v29_place_pending and self._v29_place_cube_idx is not None:\n            cube_idx = int(self._v29_place_cube_idx)\n            if not curr_env.cubes[cube_idx].is_at_goal():\n                place_pending_next = False\n                place_cube_idx_next = None\n                place_cube_z_next = None\n                release_ee_z_next = None\n                curr_target = cube_idx\n                curr_env.active_cube_override = cube_idx\n            else:\n                curr_target = cube_idx\n                if self._v29_release_ee_z is None:\n                    if _v29_release_condition(curr_env, cube_idx):\n                        if curr_env.gripper_pos is not None:\n                            release_ee_z_next = float(curr_env.gripper_pos[2])\n                        else:\n                            release_ee_z_next = float(curr_env.cubes[cube_idx].position[2])\n                else:\n                    if _v29_lift_done(curr_env, self._v29_release_ee_z):\n                        place_pending_next = False\n                        place_cube_idx_next = None\n                        place_cube_z_next = None\n                        release_ee_z_next = None\n                        curr_target = self._first_incomplete_cube_idx(curr_env)\n                        touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n                        if touched_curr is not None:\n                            curr_target = touched_curr\n                if place_pending_next:\n                    curr_env.place_cube_idx = cube_idx\n                    curr_env.place_cube_z = self._v29_place_cube_z\n                    curr_env.release_ee_z = release_ee_z_next\n                    curr_env.active_cube_override = cube_idx\n                elif curr_target is not None:\n                    curr_env.active_cube_override = int(curr_target)\n        else:\n            if curr_target is None or curr_env.cubes[int(curr_target)].is_at_goal():\n                curr_target = self._first_incomplete_cube_idx(curr_env)\n            touched_curr = self._detect_touched_cube_idx(curr_env, curr_ob)\n            if touched_curr is not None:\n                curr_target = touched_curr\n\n            if curr_target is not None:\n                cube_idx = int(curr_target)\n                entered_goal = (not prev_env.cubes[cube_idx].is_at_goal()) and curr_env.cubes[cube_idx].is_at_goal()\n                if entered_goal:\n                    place_pending_next = True\n                    place_cube_idx_next = cube_idx\n                    place_cube_z_next = float(curr_env.cubes[cube_idx].position[2])\n                    release_ee_z_next = None\n                    curr_env.place_cube_idx = cube_idx\n                    curr_env.place_cube_z = place_cube_z_next\n                    curr_env.release_ee_z = None\n                    curr_env.active_cube_override = cube_idx\n                else:\n                    curr_env.active_cube_override = cube_idx\n\n        prev_progress, _ = progress_v30(prev_env)\n        curr_progress, curr_stage = progress_v30(curr_env)\n        shaping = shaping_coef * (discount * curr_progress - prev_progress)\n        stage_penalty = self._stage_penalty_from_stage(stage=curr_stage, num_cubes=self.num_cubes, num_substages=5)\n\n        self._v24_target_cube_idx = curr_target\n        self._v29_place_pending = bool(place_pending_next)\n        self._v29_place_cube_idx = None if place_cube_idx_next is None else int(place_cube_idx_next)\n        self._v29_place_cube_z = None if place_cube_z_next is None else float(place_cube_z_next)\n        self._v29_release_ee_z = None if release_ee_z_next is None else float(release_ee_z_next)\n        return float(stage_penalty + shaping + self._success_bonus_post(curr_qpos, None, terminal_bonus))\n\n    def compute_online_reward(\n        self,\n        prev_qpos: np.ndarray,\n        curr_qpos: np.ndarray,\n        env_reward: float,\n        prev_ob: Optional[np.ndarray] = None,\n        curr_ob: Optional[np.ndarray] = None,\n        prev_gripper_gap_m: Optional[float] = None,\n        curr_gripper_gap_m: Optional[float] = None,\n        prev_left_gripper_pos: Optional[np.ndarray] = None,\n        prev_right_gripper_pos: Optional[np.ndarray] = None,\n        curr_left_gripper_pos: Optional[np.ndarray] = None,\n        curr_right_gripper_pos: Optional[np.ndarray] = None,\n        discount: float = 0.99,\n        terminal_bonus: float = 1.0,\n        shaping_coef: float = 1.0,\n    ) -> float:\n        \"\"\"Compute dense reward for an online transition via version-specific handlers.\"\"\"\n        if self.version == 'v1':\n            return self.compute_v1_online_reward(\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n            )\n        if self.version == 'v2':\n            return self.compute_v2_online_reward(\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n            )\n        if self.version == 'v3':\n            return self.compute_v3_online_reward(\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n            )\n        if self.version == 'v4':\n            return self.compute_v4_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v5':\n            return self.compute_v5_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v6':\n            return self.compute_v6_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v7':\n            return self.compute_v7_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v8':\n            return self.compute_v8_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v9':\n            return self.compute_v9_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v10':\n            return self.compute_v10_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v11':\n            return self.compute_v11_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v12':\n            return self.compute_v12_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v13':\n            return self.compute_v13_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v14':\n            return self.compute_v14_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v15':\n            return self.compute_v15_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v16':\n            return self.compute_v16_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v17':\n            return self.compute_v17_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v18':\n            return self.compute_v18_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version in ('v20', 'v21'):\n            return self.compute_v20_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v22':\n            return self.compute_v22_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v23':\n            return self.compute_v23_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v24':\n            return self.compute_v24_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v25':\n            return self.compute_v25_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v26':\n            return self.compute_v26_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v27':\n            return self.compute_v27_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v28':\n            return self.compute_v28_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v29':\n            return self.compute_v29_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        if self.version == 'v30':\n            return self.compute_v30_online_reward(\n                prev_qpos=prev_qpos,\n                curr_qpos=curr_qpos,\n                env_reward=env_reward,\n                prev_ob=prev_ob,\n                curr_ob=curr_ob,\n                prev_gripper_gap_m=prev_gripper_gap_m,\n                curr_gripper_gap_m=curr_gripper_gap_m,\n                prev_left_gripper_pos=prev_left_gripper_pos,\n                prev_right_gripper_pos=prev_right_gripper_pos,\n                curr_left_gripper_pos=curr_left_gripper_pos,\n                curr_right_gripper_pos=curr_right_gripper_pos,\n                discount=discount,\n                terminal_bonus=terminal_bonus,\n                shaping_coef=shaping_coef,\n            )\n        raise ValueError(f\"Unknown version for online rewards: {self.version}\")\n\n```",
  "video_path": "/rlwrld3/home/hyeonbin/RL/qam/artifacts/double_demo.mp4",
  "sampled_frame_indices": [
    0,
    2,
    4,
    6,
    8,
    10,
    12,
    14,
    16,
    18,
    20,
    22,
    24,
    26,
    28,
    30,
    32,
    34,
    36,
    38,
    40,
    42,
    44,
    46,
    48,
    50,
    52,
    54,
    56,
    58,
    60,
    63
  ],
  "raw_response": "```python\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\n\n# Constants from the provided environment code\nCUBE_SUCCESS_THRESHOLD = 0.04\nGRIPPER_HOME = np.array([0.425, 0.0, 0.275])\nV20_GRIPPER_GAP_OPEN_FLOOR = 0.10\nV20_GRIPPER_GAP_CLOSE_REF = 0.055\nV20_PAD_CUBE_DIST_THRESHOLD = 0.045\nV20_PAD_CUBE_AVG_DIST_THRESHOLD = 0.04\nV21_REACH_PAD_CUBE_DIST_THRESHOLD = 0.07\nV27_REACH_DIST_THRESHOLD = 0.07\nV27_REACH_XY_THRESHOLD = 0.03\nV27_REACH_Z_THRESHOLD = 0.02\nV27_GRASP_CLOSE_THRESHOLD = 0.50\nV27_GRASP_CLOSE_BASE = 0.45\nV27_GRASP_LIFT_CONFIRM = 0.02\nV28_RELEASE_OPEN_THRESHOLD = 0.45\nV28_PLACE_RELEASE_OPEN_THRESHOLD = 0.45\nV28_CLEAR_RAISE_ABOVE = 0.0",
  "parsed_json": null,
  "python_code": ""
}