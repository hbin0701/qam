{
  "system_prompt": "You are a senior robotic RL reward engineer.\nDesign reward logic that is goal-aligned and hard to game.\nReturn practical implementation-ready output.",
  "user_prompt": "You are helping design a reward specification for a robotic RL manipulation task: OGBench + QAM, \"double-cube-play-task2.\" The agent must place two cubes into their designated goal positions. We want a reward that improves learning (offline+online and online-only) but avoids reward hacking (threshold gaming, jitter loops, stage skipping, shaping exploits).\nWe prefer reward specs that are: (i) goal-aligned, (ii) hard to exploit, (iii) easy to instrument with diagnostics, and (iv) testable with invariants.\nWhen you propose terms, always include: exact definitions, state variables required, expected ranges, and the main failure modes each term might introduce.\n\nPrompt 1 \u2014 Task decomposition into stages (two cubes, two goals)\n\nGoal: define stages that are robust in a multi-object setting (avoid \"progress cancels out\" issues).\n\nDecompose \"double-cube-play-task2\" into a minimal set of stages that are (a) necessary for success, (b) observable from OGBench/QAM signals, and (c) hard to spoof.\n\nRequirements:\n- Must handle two cubes without assuming a fixed order, but you may propose an \"active cube\" selection rule if needed.\n- Each stage must have entry/exit conditions with explicit thresholds and hysteresis if relevant.\n- Include a \"stage completion\" event definition that fires once to prevent farming.\n- For each stage, list \"most common exploit\" and \"anti-exploit guard.\"\n\nOutput format (STRICT JSON ONLY; no markdown, no prose before/after):\n{\n  \"task\": \"double-cube-play-task2\",\n  \"active_cube_rule_summary\": \"string\",\n  \"stages\": [\n    {\n      \"id\": \"S0\",\n      \"name\": \"string\",\n      \"observable_signals\": [\"string\"],\n      \"condition\": {\n        \"entry\": \"string\",\n        \"exit\": \"string\"\n      },\n      \"hysteresis\": \"string\",\n      \"one_shot_event\": {\n        \"name\": \"string\",\n        \"fire_once_scope\": \"per_cube_per_episode\",\n        \"fire_condition\": \"string\"\n      },\n      \"exploit\": \"string\",\n      \"guard\": \"string\"\n    }\n  ]\n}\n\nRules:\n- Return valid JSON parseable by Python json.loads.\n- Do not wrap output in ```json fences.\n- Use stage IDs S0..Sk.\n- Include explicit numeric thresholds in entry/exit/hysteresis text where possible.\n- Keep each string field concise (<= 220 characters).\n- Keep total output under 2500 characters.",
  "video_path": "qam/artifacts/offline_progress_reward_double_sample_v25tracked.mp4",
  "raw_response": "```json\n{\n  \"task\": \"double-cube-play-task2\",\n  \"active_cube_rule_summary\": \"The active cube (C) is the one currently grasped. If no cube is grasped, C is the unplaced cube (i.e., 'C_placed_and_released' event has not fired) closest to the end-effector. If both cubes are placed, no active cube.\",\n  \"stages\": [\n    {\n      \"id\": \"S0\",\n      \"name\": \"Grasp Active Cube\",\n      \"observable_signals\": [\n        \"robot_eef_pos\",\n        \"active_cube_pos\",\n        \"gripper_open_state\",\n        \"is_grasping_active_cube\",\n        \"active_cube_placed_and_released_event_fired\"\n      ],\n      \"condition\": {\n        \"entry\": \"dist_eef_to_active_cube < 0.05 AND is_grasping_active_cube is false AND active_cube_placed_and_released_event_fired is false\",\n        \"exit\": \"is_grasping_active_cube is true\"\n      },\n      \"hysteresis\": \"dist_eef_to_active_cube must be < 0.04 to re-enter if exited by moving away from the cube.\",\n      \"one_shot_event\": {\n        \"name\": \"active_cube_grasped\",\n",
  "parsed_json": null
}