{
  "system_prompt": "You are a senior robotic RL reward engineer.\nDesign reward logic that is goal-aligned and hard to game.\nReturn practical implementation-ready output.",
  "user_prompt": "You are helping design a reward specification for a robotic RL manipulation task: OGBench + QAM, \"double-cube-play-task2.\" The agent must place two cubes into their designated goal positions. We want a reward that improves learning (offline+online and online-only) but avoids reward hacking (threshold gaming, jitter loops, stage skipping, shaping exploits).\nWe prefer reward specs that are: (i) goal-aligned, (ii) hard to exploit, (iii) easy to instrument with diagnostics, and (iv) testable with invariants.\nWhen you propose terms, always include: exact definitions, state variables required, expected ranges, and the main failure modes each term might introduce.\n\nPrompt 1 \u2014 Task decomposition into stages (two cubes, two goals)\n\nGoal: define stages that are robust in a multi-object setting (avoid \"progress cancels out\" issues).\n\nDecompose \"double-cube-play-task2\" into a minimal set of stages that are (a) necessary for success, (b) observable from OGBench/QAM signals, and (c) hard to spoof.\n\nRequirements:\n- Must handle two cubes without assuming a fixed order, but you may propose an \"active cube\" selection rule if needed.\n- Each stage must have entry/exit conditions with explicit thresholds and hysteresis if relevant.\n- Include a \"stage completion\" event definition that fires once to prevent farming.\n- For each stage, list \"most common exploit\" and \"anti-exploit guard.\"\n\nOutput format (STRICT JSON ONLY; no markdown, no prose before/after):\n{\n  \"task\": \"double-cube-play-task2\",\n  \"active_cube_rule_summary\": \"string\",\n  \"stages\": [\n    {\n      \"id\": \"S0\",\n      \"name\": \"string\",\n      \"observable_signals\": [\"string\"],\n      \"condition\": {\n        \"entry\": \"string\",\n        \"exit\": \"string\"\n      },\n      \"hysteresis\": \"string\",\n      \"one_shot_event\": {\n        \"name\": \"string\",\n        \"fire_once_scope\": \"per_cube_per_episode\",\n        \"fire_condition\": \"string\"\n      },\n      \"exploit\": \"string\",\n      \"guard\": \"string\"\n    }\n  ]\n}\n\nRules:\n- Return valid JSON parseable by Python json.loads.\n- Use stage IDs S0..Sk.\n- Include explicit numeric thresholds in entry/exit/hysteresis text where possible.",
  "video_path": "qam/artifacts/offline_progress_reward_double_sample_v25tracked.mp4",
  "raw_response": "```json\n{\n  \"task\": \"double-cube-play-task2\",\n  \"active_cube_rule_summary\": \"At each timestep, the active cube 'X' for stage progression is determined as follows: 1. If cube_1 is currently grasped (is_grasped(cube_1) is true) AND cube_1 is not yet stably placed in its goal (is_cube_1_in_goal is false), then cube_1 is the active cube. 2. Else if cube_2 is currently grasped (is_grasped(cube_2) is true) AND cube_2 is not yet stably placed in its goal (is_cube_2_in_goal is false), then cube_2 is the active cube. 3. Else if both cube_1 and cube_2 are not yet stably placed in their goals: a. If distance(robot_gripper_pos, cube_1_pos) < distance(robot_gripper_pos, cube_2_pos), then cube_1 is the active cube. b. Else, cube_2 is the active cube. 4. Else if only cube_1 is not yet stably placed in its goal, then cube_1 is the active cube. 5. Else if only cube_2 is not yet stably placed in its goal, then cube_2 is the active cube. 6. Else (both cubes are stably placed), there is no active cube for stage progression. All stage conditions below refer to this 'active cube X",
  "parsed_json": null
}